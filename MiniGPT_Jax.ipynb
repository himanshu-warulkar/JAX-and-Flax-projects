{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/himanshu-warulkar/JAX-and-Flax-projects/blob/main/MiniGPT_Jax.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tested with free Google Compute Engine Backend. No GPU required."
      ],
      "metadata": {
        "id": "_QeF9jIUwxoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "ZePpgW6jLvja"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S5olYdwfKYI-"
      },
      "outputs": [],
      "source": [
        "import jax\n",
        "import flax.linen as nn\n",
        "import jax.numpy as jnp\n",
        "from flax.training import train_state\n",
        "import optax\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as pp\n",
        "import tqdm\n",
        "import unittest\n",
        "import time\n",
        "import functools\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ],
      "metadata": {
        "id": "AhxprQ-fKq9o",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Helper functions\n",
        "dynamic_slice_vmap = jax.vmap(jax.lax.dynamic_slice, in_axes=(None, 0, None))\n",
        "\n",
        "def get_batch(random_key, data, batch_size, block_size):\n",
        "  \"\"\"Generate a batch of data of inputs x and targets y.\n",
        "\n",
        "  Args:\n",
        "    random_key (jax.random.PRNGKey): Random number generator key.\n",
        "    data (array-like): 1d JAX array of integer tokens\n",
        "    batch_size (int): Batch size.\n",
        "    block_size (int): The maximum input context length.\n",
        "\n",
        "  Returns:\n",
        "    x (array-like): 2d JAX array of shape (batch_size, block_size).\n",
        "    y (array-like): 2d JAX array of shape (batch_size, block_size).\n",
        "        x[i, j] == y[i, j-1] where j > 0.\n",
        "  \"\"\"\n",
        "  # generate a small batch of data of inputs x and targets y\n",
        "  ix = jax.random.randint(random_key, shape=(batch_size, 1), minval=0, maxval=len(data)-block_size)\n",
        "  x = dynamic_slice_vmap(data, ix, (block_size,))\n",
        "  y = dynamic_slice_vmap(data, ix+1, (block_size,))\n",
        "  return x, y\n",
        "\n",
        "def load_shakespeare_dataset():\n",
        "  with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "  data = jnp.array(encode(text))\n",
        "  n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "  train_data = data[:n]\n",
        "  eval_data = data[n:]\n",
        "  return train_data, eval_data\n",
        "\n",
        "def init_train_state(\n",
        "    model,\n",
        "    params,\n",
        "    learning_rate=1e-4,\n",
        "):\n",
        "  tx = optax.adam(learning_rate)\n",
        "  return train_state.TrainState.create(apply_fn=model.apply, params=params, tx=tx)\n",
        "\n",
        "@jax.jit\n",
        "def train_step(state, x, y):\n",
        "  \"\"\"Run one step of training.\n",
        "  Args:\n",
        "    state (jax.training.TrainState): Jax TrainState containing weights and\n",
        "      optimizer states.\n",
        "    x (array-like): 2d JAX int array of shape (batch_size, block_size).\n",
        "    y (array-like): 2d JAX int array of shape (batch_size, block_size).\n",
        "\n",
        "  Returns:\n",
        "    state (jax.training.TrainState): The new train state after applying\n",
        "      gradient descent on weights and updating optimizer states.\n",
        "    loss (float): Loss for this training step.\n",
        "  \"\"\"\n",
        "  def _loss(params):\n",
        "    predictions = state.apply_fn(params, x) # B, T, vocab_size\n",
        "    loss = optax.softmax_cross_entropy_with_integer_labels(predictions, y)\n",
        "    return loss.mean()\n",
        "  loss, grads = jax.value_and_grad(_loss)(state.params)\n",
        "  state = state.apply_gradients(grads=grads)\n",
        "  return state, loss\n",
        "\n",
        "@jax.jit\n",
        "def eval_step(state, x, y):\n",
        "  predictions = state.apply_fn(state.params, x)\n",
        "  return optax.softmax_cross_entropy_with_integer_labels(predictions, y).mean()\n",
        "\n",
        "def run_training_loop(\n",
        "    num_iterations,\n",
        "    batch_size,\n",
        "    block_size,\n",
        "    learning_rate,\n",
        "    eval_data,\n",
        "    train_data,\n",
        "    model,\n",
        "):\n",
        "  \"\"\"\n",
        "  Runs the training loop for the specified model.\n",
        "\n",
        "  Args:\n",
        "      num_iterations (int): The number of training iterations.\n",
        "      batch_size (int): The number of samples in each batch.\n",
        "      block_size (int): The size of each block (sequence length).\n",
        "      learning_rate (float): The learning rate for the optimizer.\n",
        "      eval_data (array-like): 1d JAX array of integer tokens, consisting of evaluation data.\n",
        "      train_data (array-like): 1d JAX array of integer tokens, consisting of training data.\n",
        "      model (nn.Module, optional): A Jax Model object.\n",
        "\n",
        "  Returns:\n",
        "      state: The training state with the best eval metrics.\n",
        "\n",
        "  Example:\n",
        "      >>> final_state = run_training_loop(\n",
        "      >>>     num_iterations=1000,\n",
        "      >>>     batch_size=16,\n",
        "      >>>     block_size=32,\n",
        "      >>>     learning_rate=0.001,\n",
        "      >>>     eval_data=eval_data,\n",
        "      >>>     train_data=train_data,\n",
        "      >>>     model=mini_gpt\n",
        "      >>> )\n",
        "  \"\"\"\n",
        "  random_key = jax.random.PRNGKey(0)\n",
        "  x = jnp.ones((batch_size, block_size), dtype=jnp.int16)\n",
        "  random_key, random_subkey = jax.random.split(random_key)\n",
        "  params = model.init(random_subkey, x)\n",
        "  state = init_train_state(\n",
        "      model, params, learning_rate=learning_rate)\n",
        "  predictions = state.apply_fn(state.params, x)\n",
        "  best_state = state\n",
        "  best_eval_loss = math.inf\n",
        "  for i in range(num_iterations):\n",
        "    random_key, random_subkey = jax.random.split(random_key)\n",
        "    x, y = get_batch(random_subkey, train_data, batch_size=batch_size, block_size=block_size)\n",
        "    state, loss = train_step(state, x, y)\n",
        "\n",
        "    if i % 100 == 0:\n",
        "      random_key, random_subkey = jax.random.split(random_key)\n",
        "      eval_loss = eval_step(state, *get_batch(random_subkey, eval_data, batch_size=batch_size, block_size=block_size))\n",
        "      print(f\"Step: {i}\\t train loss: {loss}\\t eval loss: {eval_loss}\")\n",
        "      if eval_loss < best_eval_loss:\n",
        "        best_eval_loss = eval_loss\n",
        "        best_state = state\n",
        "  return best_state"
      ],
      "metadata": {
        "id": "jIoEU_PAkk9T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load and tokenize dataset"
      ],
      "metadata": {
        "id": "7bhOfS-TL0iM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "print(\"length of dataset in characters: \", len(text))"
      ],
      "metadata": {
        "id": "Wqg4qFTxK3W0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "print(''.join(chars))\n",
        "print(vocab_size)\n",
        "\n",
        "# create a mapping from characters to integers\n",
        "stoi = {ch:i for i,ch in enumerate(chars)}\n",
        "itos = {i:ch for i,ch in enumerate(chars)}\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "decode = lambda l: \"\".join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "\n",
        "# Let's now split up the data into train and validation sets\n",
        "data = jnp.array(encode(text))\n",
        "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "eval_data = data[n:]"
      ],
      "metadata": {
        "id": "m-fbEt4DLNGp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Checking the performance of a simple text decoder model\n",
        "\n",
        "The SimpleDecoder below will predict the next token given a single token."
      ],
      "metadata": {
        "id": "qu4if-0aLfAh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleDecoder(nn.Module):\n",
        "  vocab_size: int\n",
        "\n",
        "  def setup(self):\n",
        "    self.token_embedding = nn.Embed(\n",
        "        num_embeddings=self.vocab_size,\n",
        "        features=self.vocab_size)\n",
        "\n",
        "  def __call__(self, x):\n",
        "    B, T = x.shape\n",
        "    return self.token_embedding(x) # B, T, vocab_size\n",
        "\n",
        "  def generate(self, start_token, max_length=20, end_token=None):\n",
        "    # Initialize the generated sequence with the start token\n",
        "    generated_sequence = [start_token]\n",
        "    current_token = start_token\n",
        "\n",
        "    for _ in range(max_length - 1):  # We already have the start token\n",
        "      # Convert the current token to a tensor\n",
        "      current_token_tensor = jnp.array([[current_token]])\n",
        "\n",
        "      # Get the token embeddings\n",
        "      token_logits = self.__call__(current_token_tensor)\n",
        "\n",
        "      # Get the token with the highest probability\n",
        "      next_token = jnp.argmax(token_logits, axis=-1)[0]\n",
        "\n",
        "      # Append the next token to the generated sequence\n",
        "      generated_sequence.append(int(next_token[0]))\n",
        "\n",
        "      # If the end token is generated, stop the generation\n",
        "      if end_token is not None and next_token[0] == end_token:\n",
        "          break\n",
        "\n",
        "      # Update the current token\n",
        "      current_token = int(next_token[0])\n",
        "\n",
        "    return generated_sequence\n",
        "\n",
        "decoder = SimpleDecoder(vocab_size=vocab_size)\n",
        "start_token = 23\n",
        "dummy = jnp.ones((4, 8), dtype=jnp.int16)\n",
        "params = decoder.init(jax.random.PRNGKey(0), dummy)\n",
        "\n",
        "# Generate a sequence\n",
        "generated_sequence = decoder.apply(params, start_token, method=decoder.generate, max_length=20)\n",
        "print(\"Generated sequence:\", decode(generated_sequence))\n"
      ],
      "metadata": {
        "id": "KYMAEy19OUsq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Generated sequence is gibberish. Let's see if it gets better when we train it."
      ],
      "metadata": {
        "id": "sl79uLx4QXd_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# You can play around the parameters here to see how that affects loss.\n",
        "num_iterations = 7000\n",
        "learning_rate = 1e-3\n",
        "num_layers = 4\n",
        "batch_size = 16\n",
        "block_size = 32\n",
        "num_heads = 4\n",
        "hidden_dim = 64\n",
        "\n",
        "decoder = SimpleDecoder(vocab_size=vocab_size)\n",
        "\n",
        "simple_decoder_state = run_training_loop(\n",
        "    num_iterations = num_iterations,\n",
        "    learning_rate = learning_rate,\n",
        "    batch_size = batch_size,\n",
        "    block_size = block_size,\n",
        "    eval_data = eval_data,\n",
        "    train_data = train_data,\n",
        "    model = decoder\n",
        ")"
      ],
      "metadata": {
        "id": "zyGvbsMHQrwF",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generated_sequence = decoder.apply(simple_decoder_state.params, start_token, method=decoder.generate, max_length=20)\n",
        "print(\"Generated sequence:\", decode(generated_sequence))\n"
      ],
      "metadata": {
        "id": "xE_VcrDJHBmz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 1 - Implement MiniGPT.\n",
        "\n",
        "* You can use off-the-shelf Flax modules like Dense, LayerNorm. You may not use Flax's SelfAttention. Instead, use AttentionTask1 provided below.\n",
        "* Note that block_size, T, input context window length are different ways to refer to the same thing."
      ],
      "metadata": {
        "id": "5jhrHj0xLB_Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# B == batch_size.\n",
        "# T == number of tokens in sequence.\n",
        "# C == hidden_dim == hidden dimension of transformer.\n",
        "# head_dim == Head dimension for each Attention head. head_dim * num_heads == C.\n",
        "\n",
        "# You can use this class for solving Task 1. We will revisit this class in Task 2.\n",
        "class AttentionTask1(nn.Module):\n",
        "  head_dim: int\n",
        "\n",
        "  def setup(self):\n",
        "    self.query = nn.Dense(features=self.head_dim, use_bias=False)\n",
        "    self.key = nn.Dense(features=self.head_dim, use_bias=False)\n",
        "    self.value = nn.Dense(features=self.head_dim, use_bias=False)\n",
        "    self.attention_impl = nn.MultiHeadDotProductAttention(\n",
        "        num_heads=1, qkv_features=self.head_dim, dropout_rate=0.)\n",
        "\n",
        "  def __call__(self, x):\n",
        "    B, T, C = x.shape\n",
        "    q = self.query(x)  # B, T, head_dim\n",
        "    k = self.key(x)  # B, T, head_dim\n",
        "    v = self.value(x)  # B, T, head_dim\n",
        "    mask = jnp.tril(jnp.ones((B, 1, T, T)))\n",
        "    return self.attention_impl(inputs_q=q, inputs_k=k, inputs_v=v, mask=mask)  # B, T, head_dim\n",
        "\n",
        "# FeedForward is given to you for free.\n",
        "class FeedForward(nn.Module):\n",
        "  hidden_dim: int\n",
        "\n",
        "  def setup(self):\n",
        "    self.f1 = nn.Dense(features=4 * self.hidden_dim)\n",
        "    self.f2 = nn.Dense(features=self.hidden_dim)\n",
        "\n",
        "  def __call__(self, x):\n",
        "    return self.f2(nn.relu(self.f1(x)))  # B, T, hidden_dim\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "  num_heads: int\n",
        "  head_dim: int\n",
        "\n",
        "  def setup(self):\n",
        "    self.heads = [AttentionTask1(self.head_dim) for _ in range(self.num_heads)]\n",
        "    self.dense = nn.Dense(self.num_heads*self.head_dim)\n",
        "\n",
        "  def __call__(self, x):\n",
        "    # Process input through each attention head\n",
        "    head_outputs = [head(x) for head in self.heads]  # List of [B, T, head_dim]\n",
        "\n",
        "    # Concatenate all head outputs along the feature dimension\n",
        "    concatenated = jnp.concatenate(head_outputs, axis=-1)  # [B, T, num_heads*head_dim]\n",
        "\n",
        "    # Project back to hidden_dim\n",
        "    return self.dense(concatenated)  # [B, T, hidden_dim]\n",
        "\n",
        "class DecoderBlock(nn.Module):\n",
        "  hidden_dim: int\n",
        "  num_heads: int\n",
        "\n",
        "  def setup(self):\n",
        "    head_dim = self.hidden_dim // self.num_heads\n",
        "    self.mha = MultiHeadAttention(num_heads=self.num_heads, head_dim=head_dim)\n",
        "    self.ffn = FeedForward(hidden_dim=self.hidden_dim)\n",
        "    self.ln1 = nn.LayerNorm()\n",
        "    self.ln2 = nn.LayerNorm()\n",
        "\n",
        "  def __call__(self, x):\n",
        "    # First residual connection with layer norm\n",
        "    attn_output = self.mha(self.ln1(x))\n",
        "    x = x + attn_output  # Residual connection\n",
        "\n",
        "    # Second residual connection with layer norm\n",
        "    ffn_output = self.ffn(self.ln2(x))\n",
        "    x = x + ffn_output  # Residual connection\n",
        "\n",
        "    return x  # [B, T, hidden_dim]\n",
        "\n",
        "class MiniGPT(nn.Module):\n",
        "  vocab_size: int\n",
        "  hidden_dim: int\n",
        "  block_size: int\n",
        "  num_layers: int\n",
        "  num_heads: int\n",
        "\n",
        "  def setup(self):\n",
        "    self.token_embedding = nn.Embed(\n",
        "        num_embeddings=self.vocab_size,\n",
        "        features=self.hidden_dim)\n",
        "    self.position_encoding = nn.Embed(\n",
        "        num_embeddings=self.block_size,\n",
        "        features=self.hidden_dim\n",
        "    )\n",
        "    self.final_dense = nn.Dense(features=self.vocab_size)\n",
        "    self.decoder_blocks = [\n",
        "        DecoderBlock(hidden_dim=self.hidden_dim, num_heads=self.num_heads)\n",
        "        for _ in range(self.num_layers)\n",
        "    ]\n",
        "    self.ln_final = nn.LayerNorm()\n",
        "\n",
        "  def __call__(self, x):\n",
        "    B, T = x.shape\n",
        "    token_embeddings = self.token_embedding(x)  # [B, T, hidden_dim]\n",
        "    position_embeddings = self.position_encoding(jnp.arange(T))[None, :, :]  # [1, T, hidden_dim]\n",
        "\n",
        "    # Combine token and position embeddings\n",
        "    x = token_embeddings + position_embeddings  # [B, T, hidden_dim]\n",
        "\n",
        "    # Process through decoder blocks\n",
        "    for block in self.decoder_blocks:\n",
        "      x = block(x)\n",
        "\n",
        "    # Final layer norm\n",
        "    x = self.ln_final(x)\n",
        "\n",
        "    return self.final_dense(x)  # [B, T, vocab_size]\n",
        "\n",
        "\n",
        "  def generate(self, random_key, params, x, max_new_tokens=50):\n",
        "    for _ in range(max_new_tokens):\n",
        "      logits = self.apply(params, x[:, -self.block_size:])\n",
        "      random_key, random_subkey = jax.random.split(random_key)\n",
        "      new_token = jax.random.categorical(random_subkey, logits[:, -1, :], axis=-1, shape=None)\n",
        "      x = jnp.concatenate([x, new_token[:, None]], axis=1)\n",
        "    return x"
      ],
      "metadata": {
        "id": "Lsjrh9ZhOgzh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# You can play around the parameters here to see how that affects loss.\n",
        "num_iterations = 4000\n",
        "learning_rate = 1e-3\n",
        "num_layers = 4\n",
        "batch_size = 16\n",
        "block_size = 32\n",
        "num_heads = 4\n",
        "hidden_dim = 128\n",
        "\n",
        "mini_gpt = MiniGPT(\n",
        "    vocab_size=vocab_size,\n",
        "    hidden_dim=hidden_dim,\n",
        "    block_size=block_size,\n",
        "    num_layers=num_layers,\n",
        "    num_heads=num_heads\n",
        ")\n",
        "\n",
        "mini_gpt_state = run_training_loop(\n",
        "    num_iterations=num_iterations,\n",
        "    learning_rate=learning_rate,\n",
        "    batch_size=batch_size,\n",
        "    block_size=block_size,\n",
        "    eval_data=eval_data,\n",
        "    train_data=train_data,\n",
        "    model=mini_gpt\n",
        ")"
      ],
      "metadata": {
        "id": "emkzC15clmUp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Uncomment below to print predictions:\n",
        "#x = jnp.zeros((1, 1), dtype=jnp.int32)\n",
        "#random_key = jax.random.PRNGKey(0)\n",
        "#tokens = mini_gpt.generate(random_key, params=mini_gpt_state.params, x=x)\n",
        "#print(decode(tokens[0].tolist()))"
      ],
      "metadata": {
        "id": "gyB7K_wM6rXw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pass this test before moving on to Task 2.\n",
        "class TestTask1(unittest.TestCase):\n",
        "\n",
        "  def test_minigpt(self):\n",
        "    # Do not change these parameters.\n",
        "    num_iterations = 4000\n",
        "    learning_rate = 1e-3\n",
        "    num_layers = 4\n",
        "    batch_size = 16\n",
        "    block_size = 32\n",
        "    num_heads = 4\n",
        "    hidden_dim = 128\n",
        "    target_loss = 1.9\n",
        "    random_key = jax.random.PRNGKey(42)\n",
        "\n",
        "    mini_gpt = MiniGPT(\n",
        "        vocab_size=vocab_size,\n",
        "        hidden_dim=hidden_dim,\n",
        "        block_size=block_size,\n",
        "        num_layers=num_layers,\n",
        "        num_heads=num_heads\n",
        "    )\n",
        "\n",
        "    train_data, eval_data = load_shakespeare_dataset()\n",
        "    mini_gpt_state = run_training_loop(\n",
        "        num_iterations = num_iterations,\n",
        "        learning_rate = learning_rate,\n",
        "        batch_size = batch_size,\n",
        "        block_size = block_size,\n",
        "        eval_data = eval_data,\n",
        "        train_data = train_data,\n",
        "        model = mini_gpt\n",
        "    )\n",
        "    eval_losses = []\n",
        "    for _ in tqdm.tqdm(range(100)):\n",
        "      random_key, random_subkey = jax.random.split(random_key)\n",
        "      x, y = get_batch(\n",
        "          random_subkey, eval_data, batch_size=batch_size, block_size=block_size)\n",
        "      batch_eval_loss = eval_step(mini_gpt_state, x, y)\n",
        "      eval_losses.append(batch_eval_loss)\n",
        "    print(f\"Average eval loss: {np.mean(eval_losses)}\")\n",
        "    self.assertTrue(np.mean(eval_losses) < target_loss)\n",
        "\n",
        "# Uncomment the test below.\n",
        "#TestTask1().test_minigpt()"
      ],
      "metadata": {
        "id": "cleRdSmmvDSs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implement the Self-Attention Jax Module\n",
        "\n",
        "\n",
        "* We are implementing a decoder-only transformer. This means that each token can only attend to previous tokens, but not future tokens."
      ],
      "metadata": {
        "id": "aezQPKA3OXLR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AttentionTask2Solution(nn.Module):\n",
        "  head_dim: int\n",
        "\n",
        "  def setup(self):\n",
        "    self.query = nn.Dense(features=self.head_dim, use_bias=False)\n",
        "    self.key = nn.Dense(features=self.head_dim, use_bias=False)\n",
        "    self.value = nn.Dense(features=self.head_dim, use_bias=False)\n",
        "\n",
        "  def __call__(self, x):\n",
        "    B, T, C = x.shape\n",
        "    q = self.query(x) # B, T, head_dim\n",
        "    k = self.key(x) # B, T, head_dim\n",
        "    wei = q @ jax.numpy.transpose(k, axes=(0, 2, 1)) # B, T, T\n",
        "    mask = jnp.tril(jnp.ones((T, T)))\n",
        "    wei = jnp.where(mask, wei, -jnp.inf)\n",
        "    wei = nn.softmax(wei / jnp.sqrt(self.head_dim), axis=-1) # B, T, T\n",
        "    return wei @ self.value(x) # B, T, C"
      ],
      "metadata": {
        "id": "r0aff872OvIw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TestAttention(unittest.TestCase):\n",
        "\n",
        "  EXPECTED_ATTENTION_ARRAY = np.array([\n",
        "    [[-0.3368626, 0.1565489, 0.96250117, 0.7116083, 0.48668504,\n",
        "      0.3070267, -0.49149823, 0.7827484, 0.4131582, 0.7505922,\n",
        "      0.90185213, -0.34802976, 1.2631372, 0.8314824, 0.45534268,\n",
        "      0.11072167],\n",
        "     [0.355573, 0.36409345, 0.19864899, 0.58222437, -0.01833684,\n",
        "      0.8821246, 0.26334122, 0.10999514, 0.69409794, 0.3437622,\n",
        "      -0.71399987, 0.6530971, 0.00235165, -0.5397035, 0.55874693,\n",
        "      -0.4885986],\n",
        "     [0.6003635, 0.34785143, -0.25671193, 0.3002994, -0.31720588,\n",
        "      1.2125036, 0.6570689, -0.22460055, 0.9200514, -0.01703957,\n",
        "      -1.5395278, 1.1767541, -0.7460983, -1.3350787, 0.61231965,\n",
        "      -1.0458561],\n",
        "     [-0.7845163, -0.5571454, 0.39112994, -0.63247937, -0.2971205,\n",
        "      0.19273886, -0.25068092, 0.5804176, 0.3952121, 0.24023446,\n",
        "      1.1744585, -1.0228857, 1.0987606, 0.90741533, 0.19215004,\n",
        "      -0.98253024]]\n",
        "    ]\n",
        "  )\n",
        "\n",
        "  def test_attention(self):\n",
        "    attention = AttentionTask2Solution(head_dim=16)\n",
        "    params = attention.init(jax.random.key(0), jnp.ones((1, 4, 8)))\n",
        "    x = jax.random.normal(key=jax.random.key(0), shape=(1, 4, 8), dtype=jnp.float32)\n",
        "    y = attention.apply(params, x)\n",
        "    self.assertTrue(np.allclose(y, self.EXPECTED_ATTENTION_ARRAY))\n",
        "\n",
        "#TestAttention().test_attention()"
      ],
      "metadata": {
        "id": "C3WxOMkcaV4M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Speed up MultiheadAttention with Einsum.\n",
        "\n"
      ],
      "metadata": {
        "id": "uAWkbRLV7gp8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttentionTask3(nn.Module):\n",
        "  num_heads: int\n",
        "  head_dim: int\n",
        "\n",
        "  def setup(self):\n",
        "    self.query = nn.Dense(features=self.num_heads * self.head_dim, use_bias=False)\n",
        "    self.key = nn.Dense(features=self.num_heads * self.head_dim, use_bias=False)\n",
        "    self.value = nn.Dense(features=self.num_heads * self.head_dim, use_bias=False)\n",
        "    self.dense = nn.Dense(features=self.num_heads * self.head_dim)\n",
        "\n",
        "  def __call__(self, x):\n",
        "    B, T, C = x.shape\n",
        "\n",
        "    # Project inputs to queries, keys, values\n",
        "    q = self.query(x)  # [B, T, num_heads * head_dim]\n",
        "    k = self.key(x)    # [B, T, num_heads * head_dim]\n",
        "    v = self.value(x)  # [B, T, num_heads * head_dim]\n",
        "\n",
        "    # Reshape to separate heads\n",
        "    q = jnp.reshape(q, (B, T, self.num_heads, self.head_dim))  # [B, T, num_heads, head_dim]\n",
        "    k = jnp.reshape(k, (B, T, self.num_heads, self.head_dim))  # [B, T, num_heads, head_dim]\n",
        "    v = jnp.reshape(v, (B, T, self.num_heads, self.head_dim))  # [B, T, num_heads, head_dim]\n",
        "\n",
        "    # Compute attention scores using einsum\n",
        "    attn_scores = jnp.einsum('bqhd,bkhd->bhqk', q, k)  # [B, num_heads, T, T]\n",
        "\n",
        "    # Scale attention scores\n",
        "    attn_scores = attn_scores / jnp.sqrt(self.head_dim)\n",
        "\n",
        "    # Create causal mask\n",
        "    mask = jnp.tril(jnp.ones((T, T)))  # [T, T]\n",
        "    mask = mask[None, None, :, :]  # [1, 1, T, T]\n",
        "    attn_scores = jnp.where(mask == 0, -1e9, attn_scores)\n",
        "\n",
        "    # Compute attention weights\n",
        "    attn_weights = nn.softmax(attn_scores, axis=-1)  # [B, num_heads, T, T]\n",
        "\n",
        "    # Apply attention to values\n",
        "    out = jnp.einsum('bhqk,bkhd->bqhd', attn_weights, v)  # [B, T, num_heads, head_dim]\n",
        "\n",
        "    # Concatenate heads and project\n",
        "    out = jnp.reshape(out, (B, T, self.num_heads * self.head_dim))  # [B, T, num_heads * head_dim]\n",
        "    out = self.dense(out)  # [B, T, num_heads * head_dim]\n",
        "\n",
        "    return out"
      ],
      "metadata": {
        "id": "oAktMykm5TMe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "batch_size = 32\n",
        "block_size = 128\n",
        "max_iters = 5000\n",
        "eval_interval = 500\n",
        "learning_rate = 3e-4\n",
        "device = 'cpu'  # or 'gpu' if available\n",
        "eval_iters = 200\n",
        "\n",
        "# Model parameters\n",
        "vocab_size = 50257  # GPT-2 vocab size\n",
        "hidden_dim = 256\n",
        "num_heads = 8\n",
        "num_layers = 6\n",
        "head_dim = hidden_dim // num_heads\n",
        "\n",
        "# Initialize model\n",
        "model = MiniGPTWithTask3(\n",
        "    vocab_size=vocab_size,\n",
        "    hidden_dim=hidden_dim,\n",
        "    block_size=block_size,\n",
        "    num_layers=num_layers,\n",
        "    num_heads=num_heads\n",
        ")\n",
        "\n",
        "# Initialize parameters\n",
        "key = jax.random.PRNGKey(0)\n",
        "key, subkey = jax.random.split(key)\n",
        "params = model.init(subkey, jnp.ones((batch_size, block_size), dtype=jnp.int32))\n",
        "\n",
        "# Optimizer\n",
        "optimizer = optax.adamw(learning_rate)\n",
        "opt_state = optimizer.init(params)\n",
        "\n",
        "# Training step\n",
        "@jax.jit\n",
        "def train_step(params, opt_state, xb, yb):\n",
        "    def loss_fn(params):\n",
        "        logits = model.apply(params, xb)\n",
        "        loss = optax.softmax_cross_entropy_with_integer_labels(\n",
        "            logits=logits.reshape(-1, logits.shape[-1]),\n",
        "            labels=yb.reshape(-1)\n",
        "        ).mean()\n",
        "        return loss\n",
        "    loss, grads = jax.value_and_grad(loss_fn)(params)\n",
        "    updates, opt_state = optimizer.update(grads, opt_state, params)\n",
        "    params = optax.apply_updates(params, updates)\n",
        "    return params, opt_state, loss\n",
        "\n",
        "# Evaluation function\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    for split in ['train', 'val']:\n",
        "        losses = []\n",
        "        for _ in range(eval_iters):\n",
        "            xb, yb = get_batch(split)\n",
        "            logits = model.apply(params, xb)\n",
        "            loss = optax.softmax_cross_entropy_with_integer_labels(\n",
        "                logits=logits.reshape(-1, logits.shape[-1]),\n",
        "                labels=yb.reshape(-1)\n",
        "            ).mean()\n",
        "            losses.append(loss)\n",
        "        out[split] = np.mean(losses)\n",
        "    return out\n",
        "\n",
        "# Training loop\n",
        "train_losses = []\n",
        "eval_losses = []\n",
        "\n",
        "for iter in range(max_iters):\n",
        "    # Get batch\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # Train step\n",
        "    params, opt_state, loss = train_step(params, opt_state, xb, yb)\n",
        "    train_losses.append(loss)\n",
        "\n",
        "    # Evaluation\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        eval_losses.append(losses['val'])\n",
        "        print(f\"Step {iter}: train loss {loss:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "a2s7pU3F0E5P",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2E1ReiJPldG1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}