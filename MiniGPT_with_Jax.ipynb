{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/himanshu-warulkar/JAX-and-Flax-projects/blob/main/MiniGPT_with_Jax.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tested with free Google Compute Engine Backend. No GPU required."
      ],
      "metadata": {
        "id": "_QeF9jIUwxoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "ZePpgW6jLvja"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S5olYdwfKYI-"
      },
      "outputs": [],
      "source": [
        "import jax\n",
        "import flax.linen as nn\n",
        "import jax.numpy as jnp\n",
        "from flax.training import train_state\n",
        "import optax\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as pp\n",
        "import tqdm\n",
        "import unittest\n",
        "import time\n",
        "import functools\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ],
      "metadata": {
        "id": "AhxprQ-fKq9o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "591dda88-bfc0-4765-8526-b4b3404c40e0",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-03-26 06:14:51--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.110.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt’\n",
            "\n",
            "\rinput.txt             0%[                    ]       0  --.-KB/s               \rinput.txt           100%[===================>]   1.06M  --.-KB/s    in 0.05s   \n",
            "\n",
            "2025-03-26 06:14:52 (22.0 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Helper functions\n",
        "dynamic_slice_vmap = jax.vmap(jax.lax.dynamic_slice, in_axes=(None, 0, None))\n",
        "\n",
        "def get_batch(random_key, data, batch_size, block_size):\n",
        "  \"\"\"Generate a batch of data of inputs x and targets y.\n",
        "\n",
        "  Args:\n",
        "    random_key (jax.random.PRNGKey): Random number generator key.\n",
        "    data (array-like): 1d JAX array of integer tokens\n",
        "    batch_size (int): Batch size.\n",
        "    block_size (int): The maximum input context length.\n",
        "\n",
        "  Returns:\n",
        "    x (array-like): 2d JAX array of shape (batch_size, block_size).\n",
        "    y (array-like): 2d JAX array of shape (batch_size, block_size).\n",
        "        x[i, j] == y[i, j-1] where j > 0.\n",
        "  \"\"\"\n",
        "  # generate a small batch of data of inputs x and targets y\n",
        "  ix = jax.random.randint(random_key, shape=(batch_size, 1), minval=0, maxval=len(data)-block_size)\n",
        "  x = dynamic_slice_vmap(data, ix, (block_size,))\n",
        "  y = dynamic_slice_vmap(data, ix+1, (block_size,))\n",
        "  return x, y\n",
        "\n",
        "def load_shakespeare_dataset():\n",
        "  with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "  data = jnp.array(encode(text))\n",
        "  n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "  train_data = data[:n]\n",
        "  eval_data = data[n:]\n",
        "  return train_data, eval_data\n",
        "\n",
        "def init_train_state(\n",
        "    model,\n",
        "    params,\n",
        "    learning_rate=1e-4,\n",
        "):\n",
        "  tx = optax.adam(learning_rate)\n",
        "  return train_state.TrainState.create(apply_fn=model.apply, params=params, tx=tx)\n",
        "\n",
        "@jax.jit\n",
        "def train_step(state, x, y):\n",
        "  \"\"\"Run one step of training.\n",
        "  Args:\n",
        "    state (jax.training.TrainState): Jax TrainState containing weights and\n",
        "      optimizer states.\n",
        "    x (array-like): 2d JAX int array of shape (batch_size, block_size).\n",
        "    y (array-like): 2d JAX int array of shape (batch_size, block_size).\n",
        "\n",
        "  Returns:\n",
        "    state (jax.training.TrainState): The new train state after applying\n",
        "      gradient descent on weights and updating optimizer states.\n",
        "    loss (float): Loss for this training step.\n",
        "  \"\"\"\n",
        "  def _loss(params):\n",
        "    predictions = state.apply_fn(params, x) # B, T, vocab_size\n",
        "    loss = optax.softmax_cross_entropy_with_integer_labels(predictions, y)\n",
        "    return loss.mean()\n",
        "  loss, grads = jax.value_and_grad(_loss)(state.params)\n",
        "  state = state.apply_gradients(grads=grads)\n",
        "  return state, loss\n",
        "\n",
        "@jax.jit\n",
        "def eval_step(state, x, y):\n",
        "  predictions = state.apply_fn(state.params, x)\n",
        "  return optax.softmax_cross_entropy_with_integer_labels(predictions, y).mean()\n",
        "\n",
        "def run_training_loop(\n",
        "    num_iterations,\n",
        "    batch_size,\n",
        "    block_size,\n",
        "    learning_rate,\n",
        "    eval_data,\n",
        "    train_data,\n",
        "    model,\n",
        "):\n",
        "  \"\"\"\n",
        "  Runs the training loop for the specified model.\n",
        "\n",
        "  Args:\n",
        "      num_iterations (int): The number of training iterations.\n",
        "      batch_size (int): The number of samples in each batch.\n",
        "      block_size (int): The size of each block (sequence length).\n",
        "      learning_rate (float): The learning rate for the optimizer.\n",
        "      eval_data (array-like): 1d JAX array of integer tokens, consisting of evaluation data.\n",
        "      train_data (array-like): 1d JAX array of integer tokens, consisting of training data.\n",
        "      model (nn.Module, optional): A Jax Model object.\n",
        "\n",
        "  Returns:\n",
        "      state: The training state with the best eval metrics.\n",
        "\n",
        "  Example:\n",
        "      >>> final_state = run_training_loop(\n",
        "      >>>     num_iterations=1000,\n",
        "      >>>     batch_size=16,\n",
        "      >>>     block_size=32,\n",
        "      >>>     learning_rate=0.001,\n",
        "      >>>     eval_data=eval_data,\n",
        "      >>>     train_data=train_data,\n",
        "      >>>     model=mini_gpt\n",
        "      >>> )\n",
        "  \"\"\"\n",
        "  random_key = jax.random.PRNGKey(0)\n",
        "  x = jnp.ones((batch_size, block_size), dtype=jnp.int16)\n",
        "  random_key, random_subkey = jax.random.split(random_key)\n",
        "  params = model.init(random_subkey, x)\n",
        "  state = init_train_state(\n",
        "      model, params, learning_rate=learning_rate)\n",
        "  predictions = state.apply_fn(state.params, x)\n",
        "  best_state = state\n",
        "  best_eval_loss = math.inf\n",
        "  for i in range(num_iterations):\n",
        "    random_key, random_subkey = jax.random.split(random_key)\n",
        "    x, y = get_batch(random_subkey, train_data, batch_size=batch_size, block_size=block_size)\n",
        "    state, loss = train_step(state, x, y)\n",
        "\n",
        "    if i % 100 == 0:\n",
        "      random_key, random_subkey = jax.random.split(random_key)\n",
        "      eval_loss = eval_step(state, *get_batch(random_subkey, eval_data, batch_size=batch_size, block_size=block_size))\n",
        "      print(f\"Step: {i}\\t train loss: {loss}\\t eval loss: {eval_loss}\")\n",
        "      if eval_loss < best_eval_loss:\n",
        "        best_eval_loss = eval_loss\n",
        "        best_state = state\n",
        "  return best_state"
      ],
      "metadata": {
        "id": "jIoEU_PAkk9T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load and tokenize dataset"
      ],
      "metadata": {
        "id": "7bhOfS-TL0iM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "print(\"length of dataset in characters: \", len(text))"
      ],
      "metadata": {
        "id": "Wqg4qFTxK3W0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "202dab5d-2ed9-4d14-f9b6-59f6ec4870d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length of dataset in characters:  1115394\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "print(''.join(chars))\n",
        "print(vocab_size)\n",
        "\n",
        "# create a mapping from characters to integers\n",
        "stoi = {ch:i for i,ch in enumerate(chars)}\n",
        "itos = {i:ch for i,ch in enumerate(chars)}\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "decode = lambda l: \"\".join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "\n",
        "# Let's now split up the data into train and validation sets\n",
        "data = jnp.array(encode(text))\n",
        "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "eval_data = data[n:]"
      ],
      "metadata": {
        "id": "m-fbEt4DLNGp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24f31ee7-af3f-4f24-efd7-690bdbb04c62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
            "65\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Warm up - Check the performance of a simple text decoder model\n",
        "\n",
        "The SimpleDecoder below will predict the next token given a single token."
      ],
      "metadata": {
        "id": "qu4if-0aLfAh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleDecoder(nn.Module):\n",
        "  vocab_size: int\n",
        "\n",
        "  def setup(self):\n",
        "    self.token_embedding = nn.Embed(\n",
        "        num_embeddings=self.vocab_size,\n",
        "        features=self.vocab_size)\n",
        "\n",
        "  def __call__(self, x):\n",
        "    B, T = x.shape\n",
        "    return self.token_embedding(x) # B, T, vocab_size\n",
        "\n",
        "  def generate(self, start_token, max_length=20, end_token=None):\n",
        "    # Initialize the generated sequence with the start token\n",
        "    generated_sequence = [start_token]\n",
        "    current_token = start_token\n",
        "\n",
        "    for _ in range(max_length - 1):  # We already have the start token\n",
        "      # Convert the current token to a tensor\n",
        "      current_token_tensor = jnp.array([[current_token]])\n",
        "\n",
        "      # Get the token embeddings\n",
        "      token_logits = self.__call__(current_token_tensor)\n",
        "\n",
        "      # Get the token with the highest probability\n",
        "      next_token = jnp.argmax(token_logits, axis=-1)[0]\n",
        "\n",
        "      # Append the next token to the generated sequence\n",
        "      generated_sequence.append(int(next_token[0]))\n",
        "\n",
        "      # If the end token is generated, stop the generation\n",
        "      if end_token is not None and next_token[0] == end_token:\n",
        "          break\n",
        "\n",
        "      # Update the current token\n",
        "      current_token = int(next_token[0])\n",
        "\n",
        "    return generated_sequence\n",
        "\n",
        "decoder = SimpleDecoder(vocab_size=vocab_size)\n",
        "start_token = 23\n",
        "dummy = jnp.ones((4, 8), dtype=jnp.int16)\n",
        "params = decoder.init(jax.random.PRNGKey(0), dummy)\n",
        "\n",
        "# Generate a sequence\n",
        "generated_sequence = decoder.apply(params, start_token, method=decoder.generate, max_length=20)\n",
        "print(\"Generated sequence:\", decode(generated_sequence))\n"
      ],
      "metadata": {
        "id": "KYMAEy19OUsq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b932d64a-1f09-4f00-b390-5ded50e07f85"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated sequence: KIrjBrjBrjBrjBrjBrjB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Generated sequence is gibberish. Let's see if it gets better when we train it."
      ],
      "metadata": {
        "id": "sl79uLx4QXd_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# You can play around the parameters here to see how that affects loss.\n",
        "num_iterations = 7000\n",
        "learning_rate = 1e-3\n",
        "num_layers = 4\n",
        "batch_size = 16\n",
        "block_size = 32\n",
        "num_heads = 4\n",
        "hidden_dim = 64\n",
        "\n",
        "decoder = SimpleDecoder(vocab_size=vocab_size)\n",
        "\n",
        "simple_decoder_state = run_training_loop(\n",
        "    num_iterations = num_iterations,\n",
        "    learning_rate = learning_rate,\n",
        "    batch_size = batch_size,\n",
        "    block_size = block_size,\n",
        "    eval_data = eval_data,\n",
        "    train_data = train_data,\n",
        "    model = decoder\n",
        ")"
      ],
      "metadata": {
        "id": "zyGvbsMHQrwF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77e562c0-b02a-4860-d2eb-de45211b5175",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step: 0\t train loss: 4.187081813812256\t eval loss: 4.182030200958252\n",
            "Step: 100\t train loss: 4.064845085144043\t eval loss: 4.06677770614624\n",
            "Step: 200\t train loss: 3.9586899280548096\t eval loss: 3.952086925506592\n",
            "Step: 300\t train loss: 3.825485944747925\t eval loss: 3.8633551597595215\n",
            "Step: 400\t train loss: 3.743715524673462\t eval loss: 3.7595314979553223\n",
            "Step: 500\t train loss: 3.635047435760498\t eval loss: 3.6645915508270264\n",
            "Step: 600\t train loss: 3.565918445587158\t eval loss: 3.587005615234375\n",
            "Step: 700\t train loss: 3.510927200317383\t eval loss: 3.5110387802124023\n",
            "Step: 800\t train loss: 3.403330087661743\t eval loss: 3.3679308891296387\n",
            "Step: 900\t train loss: 3.3363804817199707\t eval loss: 3.3412375450134277\n",
            "Step: 1000\t train loss: 3.265608549118042\t eval loss: 3.2942698001861572\n",
            "Step: 1100\t train loss: 3.1769626140594482\t eval loss: 3.175638198852539\n",
            "Step: 1200\t train loss: 3.0980184078216553\t eval loss: 3.0916907787323\n",
            "Step: 1300\t train loss: 3.0485033988952637\t eval loss: 3.1362059116363525\n",
            "Step: 1400\t train loss: 3.02640438079834\t eval loss: 3.0569634437561035\n",
            "Step: 1500\t train loss: 3.0045347213745117\t eval loss: 3.051352024078369\n",
            "Step: 1600\t train loss: 2.9174294471740723\t eval loss: 2.902665376663208\n",
            "Step: 1700\t train loss: 2.8972039222717285\t eval loss: 2.9128901958465576\n",
            "Step: 1800\t train loss: 2.8544042110443115\t eval loss: 2.9251291751861572\n",
            "Step: 1900\t train loss: 2.908815860748291\t eval loss: 2.7531111240386963\n",
            "Step: 2000\t train loss: 2.7955992221832275\t eval loss: 2.8848631381988525\n",
            "Step: 2100\t train loss: 2.813688278198242\t eval loss: 2.751432180404663\n",
            "Step: 2200\t train loss: 2.772392749786377\t eval loss: 2.7458443641662598\n",
            "Step: 2300\t train loss: 2.7488839626312256\t eval loss: 2.706265926361084\n",
            "Step: 2400\t train loss: 2.797199249267578\t eval loss: 2.718876361846924\n",
            "Step: 2500\t train loss: 2.740915298461914\t eval loss: 2.5865318775177\n",
            "Step: 2600\t train loss: 2.7066612243652344\t eval loss: 2.7102324962615967\n",
            "Step: 2700\t train loss: 2.57550311088562\t eval loss: 2.693089485168457\n",
            "Step: 2800\t train loss: 2.617443084716797\t eval loss: 2.5611860752105713\n",
            "Step: 2900\t train loss: 2.6543660163879395\t eval loss: 2.5657505989074707\n",
            "Step: 3000\t train loss: 2.6260251998901367\t eval loss: 2.5276331901550293\n",
            "Step: 3100\t train loss: 2.6793606281280518\t eval loss: 2.5542373657226562\n",
            "Step: 3200\t train loss: 2.6053037643432617\t eval loss: 2.5363733768463135\n",
            "Step: 3300\t train loss: 2.683979034423828\t eval loss: 2.587439775466919\n",
            "Step: 3400\t train loss: 2.646005392074585\t eval loss: 2.577051877975464\n",
            "Step: 3500\t train loss: 2.6169872283935547\t eval loss: 2.5438828468322754\n",
            "Step: 3600\t train loss: 2.4577267169952393\t eval loss: 2.5647740364074707\n",
            "Step: 3700\t train loss: 2.609745979309082\t eval loss: 2.6363635063171387\n",
            "Step: 3800\t train loss: 2.5565907955169678\t eval loss: 2.5401718616485596\n",
            "Step: 3900\t train loss: 2.611485004425049\t eval loss: 2.5701959133148193\n",
            "Step: 4000\t train loss: 2.4927268028259277\t eval loss: 2.5919103622436523\n",
            "Step: 4100\t train loss: 2.462908983230591\t eval loss: 2.635988235473633\n",
            "Step: 4200\t train loss: 2.5518248081207275\t eval loss: 2.5690951347351074\n",
            "Step: 4300\t train loss: 2.540949821472168\t eval loss: 2.591405153274536\n",
            "Step: 4400\t train loss: 2.482738494873047\t eval loss: 2.6471335887908936\n",
            "Step: 4500\t train loss: 2.4108195304870605\t eval loss: 2.5705549716949463\n",
            "Step: 4600\t train loss: 2.4802122116088867\t eval loss: 2.457798480987549\n",
            "Step: 4700\t train loss: 2.5316295623779297\t eval loss: 2.5431160926818848\n",
            "Step: 4800\t train loss: 2.495767116546631\t eval loss: 2.6044132709503174\n",
            "Step: 4900\t train loss: 2.4442298412323\t eval loss: 2.3635709285736084\n",
            "Step: 5000\t train loss: 2.4752309322357178\t eval loss: 2.569298505783081\n",
            "Step: 5100\t train loss: 2.450274705886841\t eval loss: 2.5510611534118652\n",
            "Step: 5200\t train loss: 2.537747621536255\t eval loss: 2.5298571586608887\n",
            "Step: 5300\t train loss: 2.475701093673706\t eval loss: 2.4912800788879395\n",
            "Step: 5400\t train loss: 2.523191213607788\t eval loss: 2.570516586303711\n",
            "Step: 5500\t train loss: 2.5310354232788086\t eval loss: 2.488771438598633\n",
            "Step: 5600\t train loss: 2.494028091430664\t eval loss: 2.4853529930114746\n",
            "Step: 5700\t train loss: 2.51924991607666\t eval loss: 2.469895362854004\n",
            "Step: 5800\t train loss: 2.4958109855651855\t eval loss: 2.5806028842926025\n",
            "Step: 5900\t train loss: 2.444451332092285\t eval loss: 2.535531520843506\n",
            "Step: 6000\t train loss: 2.503216028213501\t eval loss: 2.51741623878479\n",
            "Step: 6100\t train loss: 2.4222068786621094\t eval loss: 2.4907357692718506\n",
            "Step: 6200\t train loss: 2.4144554138183594\t eval loss: 2.5063722133636475\n",
            "Step: 6300\t train loss: 2.4849209785461426\t eval loss: 2.5326099395751953\n",
            "Step: 6400\t train loss: 2.5470902919769287\t eval loss: 2.5779449939727783\n",
            "Step: 6500\t train loss: 2.4581868648529053\t eval loss: 2.476888656616211\n",
            "Step: 6600\t train loss: 2.4541893005371094\t eval loss: 2.433610200881958\n",
            "Step: 6700\t train loss: 2.496623992919922\t eval loss: 2.4934301376342773\n",
            "Step: 6800\t train loss: 2.3890929222106934\t eval loss: 2.458296775817871\n",
            "Step: 6900\t train loss: 2.5174999237060547\t eval loss: 2.537418842315674\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generated_sequence = decoder.apply(simple_decoder_state.params, start_token, method=decoder.generate, max_length=20)\n",
        "print(\"Generated sequence:\", decode(generated_sequence))\n"
      ],
      "metadata": {
        "id": "xE_VcrDJHBmz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49adbb79-5e55-47fd-e1ea-5328823fb0d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated sequence: KI the the the the t\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 1 - Implement MiniGPT.\n",
        "\n",
        "* You can use off-the-shelf Flax modules like Dense, LayerNorm. You may not use Flax's SelfAttention. Instead, use AttentionTask1 provided below.\n",
        "* Note that block_size, T, input context window length are different ways to refer to the same thing."
      ],
      "metadata": {
        "id": "5jhrHj0xLB_Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# B == batch_size.\n",
        "# T == number of tokens in sequence.\n",
        "# C == hidden_dim == hidden dimension of transformer.\n",
        "# head_dim == Head dimension for each Attention head. head_dim * num_heads == C.\n",
        "\n",
        "# You can use this class for solving Task 1. We will revisit this class in Task 2.\n",
        "class AttentionTask1(nn.Module):\n",
        "  head_dim: int\n",
        "\n",
        "  def setup(self):\n",
        "    self.query = nn.Dense(features=self.head_dim, use_bias=False)\n",
        "    self.key = nn.Dense(features=self.head_dim, use_bias=False)\n",
        "    self.value = nn.Dense(features=self.head_dim, use_bias=False)\n",
        "    self.attention_impl = nn.MultiHeadDotProductAttention(\n",
        "        num_heads=1, qkv_features=self.head_dim, dropout_rate=0.)\n",
        "\n",
        "  def __call__(self, x):\n",
        "    B, T, C = x.shape\n",
        "    q = self.query(x)  # B, T, head_dim\n",
        "    k = self.key(x)  # B, T, head_dim\n",
        "    v = self.value(x)  # B, T, head_dim\n",
        "    mask = jnp.tril(jnp.ones((B, 1, T, T)))\n",
        "    return self.attention_impl(inputs_q=q, inputs_k=k, inputs_v=v, mask=mask)  # B, T, head_dim\n",
        "\n",
        "# FeedForward is given to you for free.\n",
        "class FeedForward(nn.Module):\n",
        "  hidden_dim: int\n",
        "\n",
        "  def setup(self):\n",
        "    self.f1 = nn.Dense(features=4 * self.hidden_dim)\n",
        "    self.f2 = nn.Dense(features=self.hidden_dim)\n",
        "\n",
        "  def __call__(self, x):\n",
        "    return self.f2(nn.relu(self.f1(x)))  # B, T, hidden_dim\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "  num_heads: int\n",
        "  head_dim: int\n",
        "\n",
        "  def setup(self):\n",
        "    self.heads = [AttentionTask1(self.head_dim) for _ in range(self.num_heads)]\n",
        "    self.dense = nn.Dense(self.num_heads*self.head_dim)\n",
        "\n",
        "  def __call__(self, x):\n",
        "    # Process input through each attention head\n",
        "    head_outputs = [head(x) for head in self.heads]  # List of [B, T, head_dim]\n",
        "\n",
        "    # Concatenate all head outputs along the feature dimension\n",
        "    concatenated = jnp.concatenate(head_outputs, axis=-1)  # [B, T, num_heads*head_dim]\n",
        "\n",
        "    # Project back to hidden_dim\n",
        "    return self.dense(concatenated)  # [B, T, hidden_dim]\n",
        "\n",
        "class DecoderBlock(nn.Module):\n",
        "  hidden_dim: int\n",
        "  num_heads: int\n",
        "\n",
        "  def setup(self):\n",
        "    head_dim = self.hidden_dim // self.num_heads\n",
        "    self.mha = MultiHeadAttention(num_heads=self.num_heads, head_dim=head_dim)\n",
        "    self.ffn = FeedForward(hidden_dim=self.hidden_dim)\n",
        "    self.ln1 = nn.LayerNorm()\n",
        "    self.ln2 = nn.LayerNorm()\n",
        "\n",
        "  def __call__(self, x):\n",
        "    # First residual connection with layer norm\n",
        "    attn_output = self.mha(self.ln1(x))\n",
        "    x = x + attn_output  # Residual connection\n",
        "\n",
        "    # Second residual connection with layer norm\n",
        "    ffn_output = self.ffn(self.ln2(x))\n",
        "    x = x + ffn_output  # Residual connection\n",
        "\n",
        "    return x  # [B, T, hidden_dim]\n",
        "\n",
        "class MiniGPT(nn.Module):\n",
        "  vocab_size: int\n",
        "  hidden_dim: int\n",
        "  block_size: int\n",
        "  num_layers: int\n",
        "  num_heads: int\n",
        "\n",
        "  def setup(self):\n",
        "    self.token_embedding = nn.Embed(\n",
        "        num_embeddings=self.vocab_size,\n",
        "        features=self.hidden_dim)\n",
        "    self.position_encoding = nn.Embed(\n",
        "        num_embeddings=self.block_size,\n",
        "        features=self.hidden_dim\n",
        "    )\n",
        "    self.final_dense = nn.Dense(features=self.vocab_size)\n",
        "    self.decoder_blocks = [\n",
        "        DecoderBlock(hidden_dim=self.hidden_dim, num_heads=self.num_heads)\n",
        "        for _ in range(self.num_layers)\n",
        "    ]\n",
        "    self.ln_final = nn.LayerNorm()\n",
        "\n",
        "  def __call__(self, x):\n",
        "    B, T = x.shape\n",
        "    token_embeddings = self.token_embedding(x)  # [B, T, hidden_dim]\n",
        "    position_embeddings = self.position_encoding(jnp.arange(T))[None, :, :]  # [1, T, hidden_dim]\n",
        "\n",
        "    # Combine token and position embeddings\n",
        "    x = token_embeddings + position_embeddings  # [B, T, hidden_dim]\n",
        "\n",
        "    # Process through decoder blocks\n",
        "    for block in self.decoder_blocks:\n",
        "      x = block(x)\n",
        "\n",
        "    # Final layer norm\n",
        "    x = self.ln_final(x)\n",
        "\n",
        "    return self.final_dense(x)  # [B, T, vocab_size]\n",
        "\n",
        "\n",
        "  def generate(self, random_key, params, x, max_new_tokens=50):\n",
        "    for _ in range(max_new_tokens):\n",
        "      logits = self.apply(params, x[:, -self.block_size:])\n",
        "      random_key, random_subkey = jax.random.split(random_key)\n",
        "      new_token = jax.random.categorical(random_subkey, logits[:, -1, :], axis=-1, shape=None)\n",
        "      x = jnp.concatenate([x, new_token[:, None]], axis=1)\n",
        "    return x"
      ],
      "metadata": {
        "id": "Lsjrh9ZhOgzh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# You can play around the parameters here to see how that affects loss.\n",
        "num_iterations = 4000\n",
        "learning_rate = 1e-3\n",
        "num_layers = 4\n",
        "batch_size = 16\n",
        "block_size = 32\n",
        "num_heads = 4\n",
        "hidden_dim = 128\n",
        "\n",
        "mini_gpt = MiniGPT(\n",
        "    vocab_size=vocab_size,\n",
        "    hidden_dim=hidden_dim,\n",
        "    block_size=block_size,\n",
        "    num_layers=num_layers,\n",
        "    num_heads=num_heads\n",
        ")\n",
        "\n",
        "mini_gpt_state = run_training_loop(\n",
        "    num_iterations=num_iterations,\n",
        "    learning_rate=learning_rate,\n",
        "    batch_size=batch_size,\n",
        "    block_size=block_size,\n",
        "    eval_data=eval_data,\n",
        "    train_data=train_data,\n",
        "    model=mini_gpt\n",
        ")"
      ],
      "metadata": {
        "id": "emkzC15clmUp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6a90ade-599a-4a42-8252-2d504a744f47"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step: 0\t train loss: 4.636037349700928\t eval loss: 3.8612051010131836\n",
            "Step: 100\t train loss: 2.761054277420044\t eval loss: 2.5773842334747314\n",
            "Step: 200\t train loss: 2.51499342918396\t eval loss: 2.4729771614074707\n",
            "Step: 300\t train loss: 2.2913033962249756\t eval loss: 2.4476115703582764\n",
            "Step: 400\t train loss: 2.2185070514678955\t eval loss: 2.341761350631714\n",
            "Step: 500\t train loss: 2.103759288787842\t eval loss: 2.224623441696167\n",
            "Step: 600\t train loss: 2.1078989505767822\t eval loss: 2.3080012798309326\n",
            "Step: 700\t train loss: 2.245616912841797\t eval loss: 2.1856136322021484\n",
            "Step: 800\t train loss: 2.0958659648895264\t eval loss: 2.043236494064331\n",
            "Step: 900\t train loss: 2.01725172996521\t eval loss: 2.04413104057312\n",
            "Step: 1000\t train loss: 1.8883870840072632\t eval loss: 2.1730215549468994\n",
            "Step: 1100\t train loss: 1.9125239849090576\t eval loss: 1.9835453033447266\n",
            "Step: 1200\t train loss: 1.973968505859375\t eval loss: 1.9020031690597534\n",
            "Step: 1300\t train loss: 1.8379484415054321\t eval loss: 2.0265631675720215\n",
            "Step: 1400\t train loss: 1.896957278251648\t eval loss: 2.0503294467926025\n",
            "Step: 1500\t train loss: 1.9330224990844727\t eval loss: 2.096245288848877\n",
            "Step: 1600\t train loss: 1.795131802558899\t eval loss: 1.8485461473464966\n",
            "Step: 1700\t train loss: 1.7436254024505615\t eval loss: 1.9716602563858032\n",
            "Step: 1800\t train loss: 1.6987637281417847\t eval loss: 2.116537570953369\n",
            "Step: 1900\t train loss: 1.863325834274292\t eval loss: 1.8115551471710205\n",
            "Step: 2000\t train loss: 1.9445080757141113\t eval loss: 2.023185968399048\n",
            "Step: 2100\t train loss: 1.9024759531021118\t eval loss: 1.8705247640609741\n",
            "Step: 2200\t train loss: 1.7917059659957886\t eval loss: 1.8837902545928955\n",
            "Step: 2300\t train loss: 1.746579647064209\t eval loss: 1.8975361585617065\n",
            "Step: 2400\t train loss: 1.735876202583313\t eval loss: 1.881200909614563\n",
            "Step: 2500\t train loss: 1.959524393081665\t eval loss: 1.7438874244689941\n",
            "Step: 2600\t train loss: 1.823598027229309\t eval loss: 1.9953150749206543\n",
            "Step: 2700\t train loss: 1.6298985481262207\t eval loss: 1.8769328594207764\n",
            "Step: 2800\t train loss: 1.682288408279419\t eval loss: 1.7787045240402222\n",
            "Step: 2900\t train loss: 1.6468634605407715\t eval loss: 1.7985777854919434\n",
            "Step: 3000\t train loss: 1.6832318305969238\t eval loss: 1.7450759410858154\n",
            "Step: 3100\t train loss: 1.7387874126434326\t eval loss: 1.776960015296936\n",
            "Step: 3200\t train loss: 1.7218471765518188\t eval loss: 1.7549728155136108\n",
            "Step: 3300\t train loss: 1.7413265705108643\t eval loss: 1.7992780208587646\n",
            "Step: 3400\t train loss: 1.687448263168335\t eval loss: 1.8700337409973145\n",
            "Step: 3500\t train loss: 1.6945545673370361\t eval loss: 1.7160725593566895\n",
            "Step: 3600\t train loss: 1.587894320487976\t eval loss: 1.708038568496704\n",
            "Step: 3700\t train loss: 1.6723703145980835\t eval loss: 1.8848154544830322\n",
            "Step: 3800\t train loss: 1.6177682876586914\t eval loss: 1.7265715599060059\n",
            "Step: 3900\t train loss: 1.6156960725784302\t eval loss: 1.9318103790283203\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Uncomment below to print predictions:\n",
        "#x = jnp.zeros((1, 1), dtype=jnp.int32)\n",
        "#random_key = jax.random.PRNGKey(0)\n",
        "#tokens = mini_gpt.generate(random_key, params=mini_gpt_state.params, x=x)\n",
        "#print(decode(tokens[0].tolist()))"
      ],
      "metadata": {
        "id": "gyB7K_wM6rXw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2d5497e-5d62-41a7-a1ee-0d2d6c1c1806"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Of from we elde is have risones? I tell tell heavy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Pass this test before moving on to Task 2.\n",
        "class TestTask1(unittest.TestCase):\n",
        "\n",
        "  def test_minigpt(self):\n",
        "    # Do not change these parameters.\n",
        "    num_iterations = 4000\n",
        "    learning_rate = 1e-3\n",
        "    num_layers = 4\n",
        "    batch_size = 16\n",
        "    block_size = 32\n",
        "    num_heads = 4\n",
        "    hidden_dim = 128\n",
        "    target_loss = 1.9\n",
        "    random_key = jax.random.PRNGKey(42)\n",
        "\n",
        "    mini_gpt = MiniGPT(\n",
        "        vocab_size=vocab_size,\n",
        "        hidden_dim=hidden_dim,\n",
        "        block_size=block_size,\n",
        "        num_layers=num_layers,\n",
        "        num_heads=num_heads\n",
        "    )\n",
        "\n",
        "    train_data, eval_data = load_shakespeare_dataset()\n",
        "    mini_gpt_state = run_training_loop(\n",
        "        num_iterations = num_iterations,\n",
        "        learning_rate = learning_rate,\n",
        "        batch_size = batch_size,\n",
        "        block_size = block_size,\n",
        "        eval_data = eval_data,\n",
        "        train_data = train_data,\n",
        "        model = mini_gpt\n",
        "    )\n",
        "    eval_losses = []\n",
        "    for _ in tqdm.tqdm(range(100)):\n",
        "      random_key, random_subkey = jax.random.split(random_key)\n",
        "      x, y = get_batch(\n",
        "          random_subkey, eval_data, batch_size=batch_size, block_size=block_size)\n",
        "      batch_eval_loss = eval_step(mini_gpt_state, x, y)\n",
        "      eval_losses.append(batch_eval_loss)\n",
        "    print(f\"Average eval loss: {np.mean(eval_losses)}\")\n",
        "    self.assertTrue(np.mean(eval_losses) < target_loss)\n",
        "\n",
        "# Uncomment the test below.\n",
        "#TestTask1().test_minigpt()"
      ],
      "metadata": {
        "id": "cleRdSmmvDSs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54545b22-716e-4f7d-a905-0b85ed3a7940"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step: 0\t train loss: 4.636037349700928\t eval loss: 3.8612051010131836\n",
            "Step: 100\t train loss: 2.761054277420044\t eval loss: 2.5773842334747314\n",
            "Step: 200\t train loss: 2.51499342918396\t eval loss: 2.4729771614074707\n",
            "Step: 300\t train loss: 2.2913033962249756\t eval loss: 2.4476115703582764\n",
            "Step: 400\t train loss: 2.2185070514678955\t eval loss: 2.341761350631714\n",
            "Step: 500\t train loss: 2.103759288787842\t eval loss: 2.224623441696167\n",
            "Step: 600\t train loss: 2.1078989505767822\t eval loss: 2.3080012798309326\n",
            "Step: 700\t train loss: 2.245616912841797\t eval loss: 2.1856136322021484\n",
            "Step: 800\t train loss: 2.0958659648895264\t eval loss: 2.043236494064331\n",
            "Step: 900\t train loss: 2.01725172996521\t eval loss: 2.04413104057312\n",
            "Step: 1000\t train loss: 1.8883870840072632\t eval loss: 2.1730215549468994\n",
            "Step: 1100\t train loss: 1.9125239849090576\t eval loss: 1.9835453033447266\n",
            "Step: 1200\t train loss: 1.973968505859375\t eval loss: 1.9020031690597534\n",
            "Step: 1300\t train loss: 1.8379484415054321\t eval loss: 2.0265631675720215\n",
            "Step: 1400\t train loss: 1.896957278251648\t eval loss: 2.0503294467926025\n",
            "Step: 1500\t train loss: 1.9330224990844727\t eval loss: 2.096245288848877\n",
            "Step: 1600\t train loss: 1.795131802558899\t eval loss: 1.8485461473464966\n",
            "Step: 1700\t train loss: 1.7436254024505615\t eval loss: 1.9716602563858032\n",
            "Step: 1800\t train loss: 1.6987637281417847\t eval loss: 2.116537570953369\n",
            "Step: 1900\t train loss: 1.863325834274292\t eval loss: 1.8115551471710205\n",
            "Step: 2000\t train loss: 1.9445080757141113\t eval loss: 2.023185968399048\n",
            "Step: 2100\t train loss: 1.9024759531021118\t eval loss: 1.8705247640609741\n",
            "Step: 2200\t train loss: 1.7917059659957886\t eval loss: 1.8837902545928955\n",
            "Step: 2300\t train loss: 1.746579647064209\t eval loss: 1.8975361585617065\n",
            "Step: 2400\t train loss: 1.735876202583313\t eval loss: 1.881200909614563\n",
            "Step: 2500\t train loss: 1.959524393081665\t eval loss: 1.7438874244689941\n",
            "Step: 2600\t train loss: 1.823598027229309\t eval loss: 1.9953150749206543\n",
            "Step: 2700\t train loss: 1.6298985481262207\t eval loss: 1.8769328594207764\n",
            "Step: 2800\t train loss: 1.682288408279419\t eval loss: 1.7787045240402222\n",
            "Step: 2900\t train loss: 1.6468634605407715\t eval loss: 1.7985777854919434\n",
            "Step: 3000\t train loss: 1.6832318305969238\t eval loss: 1.7450759410858154\n",
            "Step: 3100\t train loss: 1.7387874126434326\t eval loss: 1.776960015296936\n",
            "Step: 3200\t train loss: 1.7218471765518188\t eval loss: 1.7549728155136108\n",
            "Step: 3300\t train loss: 1.7413265705108643\t eval loss: 1.7992780208587646\n",
            "Step: 3400\t train loss: 1.687448263168335\t eval loss: 1.8700337409973145\n",
            "Step: 3500\t train loss: 1.6945545673370361\t eval loss: 1.7160725593566895\n",
            "Step: 3600\t train loss: 1.587894320487976\t eval loss: 1.708038568496704\n",
            "Step: 3700\t train loss: 1.6723703145980835\t eval loss: 1.8848154544830322\n",
            "Step: 3800\t train loss: 1.6177682876586914\t eval loss: 1.7265715599060059\n",
            "Step: 3900\t train loss: 1.6156960725784302\t eval loss: 1.9318103790283203\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:02<00:00, 38.14it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average eval loss: 1.8029732704162598\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 2 - implement the Self-Attention Jax Module\n",
        "\n",
        "Your task is to implement Attention without using Flax's built-in nn.MultiHeadDotProductAttention module. Fill in the TODO section below.\n",
        "\n",
        "Things to keep in mind:\n",
        "\n",
        "* We are implementing a decoder-only transformer. This means that each token can only attend to previous tokens, but not future tokens."
      ],
      "metadata": {
        "id": "aezQPKA3OXLR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AttentionTask2Solution(nn.Module):\n",
        "  head_dim: int\n",
        "\n",
        "  def setup(self):\n",
        "    self.query = nn.Dense(features=self.head_dim, use_bias=False)\n",
        "    self.key = nn.Dense(features=self.head_dim, use_bias=False)\n",
        "    self.value = nn.Dense(features=self.head_dim, use_bias=False)\n",
        "\n",
        "  def __call__(self, x):\n",
        "    B, T, C = x.shape\n",
        "    q = self.query(x) # B, T, head_dim\n",
        "    k = self.key(x) # B, T, head_dim\n",
        "    wei = q @ jax.numpy.transpose(k, axes=(0, 2, 1)) # B, T, T\n",
        "    mask = jnp.tril(jnp.ones((T, T)))\n",
        "    wei = jnp.where(mask, wei, -jnp.inf)\n",
        "    wei = nn.softmax(wei / jnp.sqrt(self.head_dim), axis=-1) # B, T, T\n",
        "    return wei @ self.value(x) # B, T, C"
      ],
      "metadata": {
        "id": "r0aff872OvIw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TestAttention(unittest.TestCase):\n",
        "\n",
        "  EXPECTED_ATTENTION_ARRAY = np.array([\n",
        "    [[-0.3368626, 0.1565489, 0.96250117, 0.7116083, 0.48668504,\n",
        "      0.3070267, -0.49149823, 0.7827484, 0.4131582, 0.7505922,\n",
        "      0.90185213, -0.34802976, 1.2631372, 0.8314824, 0.45534268,\n",
        "      0.11072167],\n",
        "     [0.355573, 0.36409345, 0.19864899, 0.58222437, -0.01833684,\n",
        "      0.8821246, 0.26334122, 0.10999514, 0.69409794, 0.3437622,\n",
        "      -0.71399987, 0.6530971, 0.00235165, -0.5397035, 0.55874693,\n",
        "      -0.4885986],\n",
        "     [0.6003635, 0.34785143, -0.25671193, 0.3002994, -0.31720588,\n",
        "      1.2125036, 0.6570689, -0.22460055, 0.9200514, -0.01703957,\n",
        "      -1.5395278, 1.1767541, -0.7460983, -1.3350787, 0.61231965,\n",
        "      -1.0458561],\n",
        "     [-0.7845163, -0.5571454, 0.39112994, -0.63247937, -0.2971205,\n",
        "      0.19273886, -0.25068092, 0.5804176, 0.3952121, 0.24023446,\n",
        "      1.1744585, -1.0228857, 1.0987606, 0.90741533, 0.19215004,\n",
        "      -0.98253024]]\n",
        "    ]\n",
        "  )\n",
        "\n",
        "  def test_attention(self):\n",
        "    attention = AttentionTask2Solution(head_dim=16)\n",
        "    params = attention.init(jax.random.key(0), jnp.ones((1, 4, 8)))\n",
        "    x = jax.random.normal(key=jax.random.key(0), shape=(1, 4, 8), dtype=jnp.float32)\n",
        "    y = attention.apply(params, x)\n",
        "    self.assertTrue(np.allclose(y, self.EXPECTED_ATTENTION_ARRAY))\n",
        "\n",
        "#TestAttention().test_attention()"
      ],
      "metadata": {
        "id": "C3WxOMkcaV4M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 3 Speed up MultiheadAttention with Einsum.\n",
        "\n",
        "Please finish task 2 first before doing this task."
      ],
      "metadata": {
        "id": "uAWkbRLV7gp8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttentionTask3(nn.Module):\n",
        "  num_heads: int\n",
        "  head_dim: int\n",
        "\n",
        "  def setup(self):\n",
        "    self.query = nn.Dense(features=self.num_heads * self.head_dim, use_bias=False)\n",
        "    self.key = nn.Dense(features=self.num_heads * self.head_dim, use_bias=False)\n",
        "    self.value = nn.Dense(features=self.num_heads * self.head_dim, use_bias=False)\n",
        "    self.dense = nn.Dense(features=self.num_heads * self.head_dim)\n",
        "\n",
        "  def __call__(self, x):\n",
        "    B, T, C = x.shape\n",
        "\n",
        "    # Project inputs to queries, keys, values\n",
        "    q = self.query(x)  # [B, T, num_heads * head_dim]\n",
        "    k = self.key(x)    # [B, T, num_heads * head_dim]\n",
        "    v = self.value(x)  # [B, T, num_heads * head_dim]\n",
        "\n",
        "    # Reshape to separate heads\n",
        "    q = jnp.reshape(q, (B, T, self.num_heads, self.head_dim))  # [B, T, num_heads, head_dim]\n",
        "    k = jnp.reshape(k, (B, T, self.num_heads, self.head_dim))  # [B, T, num_heads, head_dim]\n",
        "    v = jnp.reshape(v, (B, T, self.num_heads, self.head_dim))  # [B, T, num_heads, head_dim]\n",
        "\n",
        "    # Compute attention scores using einsum\n",
        "    attn_scores = jnp.einsum('bqhd,bkhd->bhqk', q, k)  # [B, num_heads, T, T]\n",
        "\n",
        "    # Scale attention scores\n",
        "    attn_scores = attn_scores / jnp.sqrt(self.head_dim)\n",
        "\n",
        "    # Create causal mask\n",
        "    mask = jnp.tril(jnp.ones((T, T)))  # [T, T]\n",
        "    mask = mask[None, None, :, :]  # [1, 1, T, T]\n",
        "    attn_scores = jnp.where(mask == 0, -1e9, attn_scores)\n",
        "\n",
        "    # Compute attention weights\n",
        "    attn_weights = nn.softmax(attn_scores, axis=-1)  # [B, num_heads, T, T]\n",
        "\n",
        "    # Apply attention to values\n",
        "    out = jnp.einsum('bhqk,bkhd->bqhd', attn_weights, v)  # [B, T, num_heads, head_dim]\n",
        "\n",
        "    # Concatenate heads and project\n",
        "    out = jnp.reshape(out, (B, T, self.num_heads * self.head_dim))  # [B, T, num_heads * head_dim]\n",
        "    out = self.dense(out)  # [B, T, num_heads * head_dim]\n",
        "\n",
        "    return out"
      ],
      "metadata": {
        "id": "oAktMykm5TMe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "batch_size = 32\n",
        "block_size = 128\n",
        "max_iters = 5000\n",
        "eval_interval = 500\n",
        "learning_rate = 3e-4\n",
        "device = 'cpu'  # or 'gpu' if available\n",
        "eval_iters = 200\n",
        "\n",
        "# Model parameters\n",
        "vocab_size = 50257  # GPT-2 vocab size\n",
        "hidden_dim = 256\n",
        "num_heads = 8\n",
        "num_layers = 6\n",
        "head_dim = hidden_dim // num_heads\n",
        "\n",
        "# Initialize model\n",
        "model = MiniGPTWithTask3(\n",
        "    vocab_size=vocab_size,\n",
        "    hidden_dim=hidden_dim,\n",
        "    block_size=block_size,\n",
        "    num_layers=num_layers,\n",
        "    num_heads=num_heads\n",
        ")\n",
        "\n",
        "# Initialize parameters\n",
        "key = jax.random.PRNGKey(0)\n",
        "key, subkey = jax.random.split(key)\n",
        "params = model.init(subkey, jnp.ones((batch_size, block_size), dtype=jnp.int32))\n",
        "\n",
        "# Optimizer\n",
        "optimizer = optax.adamw(learning_rate)\n",
        "opt_state = optimizer.init(params)\n",
        "\n",
        "# Training step\n",
        "@jax.jit\n",
        "def train_step(params, opt_state, xb, yb):\n",
        "    def loss_fn(params):\n",
        "        logits = model.apply(params, xb)\n",
        "        loss = optax.softmax_cross_entropy_with_integer_labels(\n",
        "            logits=logits.reshape(-1, logits.shape[-1]),\n",
        "            labels=yb.reshape(-1)\n",
        "        ).mean()\n",
        "        return loss\n",
        "    loss, grads = jax.value_and_grad(loss_fn)(params)\n",
        "    updates, opt_state = optimizer.update(grads, opt_state, params)\n",
        "    params = optax.apply_updates(params, updates)\n",
        "    return params, opt_state, loss\n",
        "\n",
        "# Evaluation function\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    for split in ['train', 'val']:\n",
        "        losses = []\n",
        "        for _ in range(eval_iters):\n",
        "            xb, yb = get_batch(split)\n",
        "            logits = model.apply(params, xb)\n",
        "            loss = optax.softmax_cross_entropy_with_integer_labels(\n",
        "                logits=logits.reshape(-1, logits.shape[-1]),\n",
        "                labels=yb.reshape(-1)\n",
        "            ).mean()\n",
        "            losses.append(loss)\n",
        "        out[split] = np.mean(losses)\n",
        "    return out\n",
        "\n",
        "# Training loop\n",
        "train_losses = []\n",
        "eval_losses = []\n",
        "\n",
        "for iter in range(max_iters):\n",
        "    # Get batch\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # Train step\n",
        "    params, opt_state, loss = train_step(params, opt_state, xb, yb)\n",
        "    train_losses.append(loss)\n",
        "\n",
        "    # Evaluation\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        eval_losses.append(losses['val'])\n",
        "        print(f\"Step {iter}: train loss {loss:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "# Compare with your original results"
      ],
      "metadata": {
        "id": "a2s7pU3F0E5P",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "outputId": "49033ddc-32bf-43be-d3b3-841a5390d6bc",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "get_batch() missing 3 required positional arguments: 'data', 'batch_size', and 'block_size'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-e1e2fa40d9fd>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0miter\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_iters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0;31m# Get batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m     \u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0;31m# Train step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: get_batch() missing 3 required positional arguments: 'data', 'batch_size', and 'block_size'"
          ]
        }
      ]
    }
  ]
}