{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOzX8UHQImfoEVOIRQAsGC+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/himanshu-warulkar/JAX-and-Flax-projects/blob/main/Mini_Language_Model_with_JAX_and_Flax.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "id": "REJkMjvLPhEV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MTBVNo8OLDNt"
      },
      "outputs": [],
      "source": [
        "# Implementing a Mini Language Model with JAX and Flax\n",
        "\n",
        "'''\n",
        "This notebook demonstrates how to implement and train a small language model using JAX and Flax. We'll use the **WikiText-2 dataset** for training and include educational tasks to reinforce key concepts.\n",
        "\n",
        "\n",
        "'''\n",
        "## Imports\n",
        "\n",
        "\n",
        "import jax\n",
        "import flax.linen as nn\n",
        "import jax.numpy as jnp\n",
        "from flax.training import train_state\n",
        "import optax\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from datasets import load_dataset\n",
        "import tqdm\n",
        "import unittest\n",
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load and Tokenize Datasets\n",
        "### Download and Preprocess WikiText-2"
      ],
      "metadata": {
        "id": "JCEKmo3aLrgX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load WikiText-2 dataset\n",
        "dataset = load_dataset(\"wikitext\", \"wikitext-2-v1\")\n",
        "text = \"\\n\".join(dataset[\"train\"][\"text\"][:1000])  # Use a subset for faster training\n",
        "\n",
        "# Create character-level tokenizer\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "stoi = {ch:i for i, ch in enumerate(chars)}\n",
        "itos = {i:ch for i, ch in enumerate(chars)}\n",
        "encode = lambda s: [stoi[c] for c in s]\n",
        "decode = lambda l: \"\".join([itos[i] for i in l])\n",
        "\n",
        "# Split into train/validation\n",
        "data = jnp.array(encode(text))\n",
        "n = int(0.9 * len(data))\n",
        "train_data, eval_data = data[:n], data[n:]\n",
        "print(f\"Train size: {len(train_data)}, Val size: {len(eval_data)}\")"
      ],
      "metadata": {
        "id": "mxZHplQTL3m8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Helper Functions\n",
        "### Batch Generator"
      ],
      "metadata": {
        "id": "EBM-YKtTL-pG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_batch(rng, data, batch_size, block_size):\n",
        "    ix = jax.random.randint(rng, (batch_size,), 0, len(data)-block_size)\n",
        "    x = jnp.stack([data[i:i+block_size] for i in ix])\n",
        "    y = jnp.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    return x, y"
      ],
      "metadata": {
        "id": "f_CyuoFrMFdz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training Loop"
      ],
      "metadata": {
        "id": "l35nbzvhMJDW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_step(state, x, y):\n",
        "    def loss_fn(params):\n",
        "        logits = state.apply_fn(params, x)\n",
        "        loss = optax.softmax_cross_entropy_with_integer_labels(logits, y).mean()\n",
        "        return loss\n",
        "    loss, grads = jax.value_and_grad(loss_fn)(state.params)\n",
        "    return state.apply_gradients(grads=grads), loss"
      ],
      "metadata": {
        "id": "JbAjlaRFMMEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 1: Implement Positional Encoding\n",
        "\n",
        "### Objective: Add positional embeddings to the token embeddings in the model.\n",
        "### Hint: Use nn.Embed to create a positional encoding table."
      ],
      "metadata": {
        "id": "LraQ8h2WMQIc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Task1Model(nn.Module):\n",
        "    vocab_size: int\n",
        "    hidden_dim: int\n",
        "    block_size: int\n",
        "\n",
        "    def setup(self):\n",
        "        self.token_embed = nn.Embed(self.vocab_size, self.hidden_dim)\n",
        "        # TODO: Add positional encoding here\n",
        "\n",
        "    def __call__(self, x):\n",
        "        B, T = x.shape\n",
        "        tok_emb = self.token_embed(x)  # (B, T, hidden_dim)\n",
        "        # TODO: Add positional embeddings\n",
        "        return tok_emb  # Modify this"
      ],
      "metadata": {
        "id": "68ShgKX3MW4L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 2: Implement Multi-Head Attention\n",
        "\n",
        "### Objective: Create a multi-head attention layer without using Flax's built-in modules.\n",
        "#### Hint: Split queries, keys, and values into multiple heads."
      ],
      "metadata": {
        "id": "lDE1FokbMcs7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    num_heads: int\n",
        "    head_dim: int\n",
        "\n",
        "    def setup(self):\n",
        "        # TODO: Initialize query, key, value projections\n",
        "        pass\n",
        "\n",
        "    def __call__(self, x):\n",
        "        B, T, C = x.shape\n",
        "        # TODO: Split into heads, compute attention, concatenate\n",
        "        return x  # Modify this"
      ],
      "metadata": {
        "id": "39-VXsmoMeSv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 3: Optimize Training with JIT\n",
        "### Objective: Use jax.jit to compile the training step for faster execution.\n",
        "### Hint: Decorate train_step with @jax.jit."
      ],
      "metadata": {
        "id": "PLGm84YSMqUI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@jax.jit  # <-- Add this decorator\n",
        "def train_step(state, x, y):\n",
        "    def loss_fn(params):\n",
        "        #TODO\n",
        "    #return state.apply_gradients(grads=grads), loss"
      ],
      "metadata": {
        "id": "_pcLD-jkMwrU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training and Evaluation\n",
        "### Model Initialization"
      ],
      "metadata": {
        "id": "s_XXKipgM8IR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Task1Model(vocab_size=vocab_size, hidden_dim=128, block_size=32)\n",
        "rng = jax.random.PRNGKey(0)\n",
        "x = jnp.ones((4, 32), dtype=jnp.int32)\n",
        "params = model.init(rng, x)\n",
        "state = train_state.TrainState.create(\n",
        "    apply_fn=model.apply, params=params, tx=optax.adam(1e-3)\n",
        ")\n",
        "\n",
        "# Training loop\n",
        "for step in range(1000):\n",
        "    rng, subkey = jax.random.split(rng)\n",
        "    x, y = get_batch(subkey, train_data, batch_size=32, block_size=32)\n",
        "    state, loss = train_step(state, x, y)\n",
        "    if step % 100 == 0:\n",
        "        print(f\"Step {step}, Loss: {loss:.4f}\")"
      ],
      "metadata": {
        "id": "g5rYIK2ONENV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Appendix: **Solutions**\n",
        "## Task 1 Solution"
      ],
      "metadata": {
        "id": "1eTz0uB-NOix"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Task1Solution(nn.Module):\n",
        "    vocab_size: int\n",
        "    hidden_dim: int\n",
        "    block_size: int\n",
        "\n",
        "    def setup(self):\n",
        "        self.token_embed = nn.Embed(self.vocab_size, self.hidden_dim)\n",
        "        self.pos_embed = nn.Embed(self.block_size, self.hidden_dim)\n",
        "\n",
        "    def __call__(self, x):\n",
        "        B, T = x.shape\n",
        "        tok_emb = self.token_embed(x)\n",
        "        pos = self.pos_embed(jnp.arange(T))\n",
        "        return tok_emb + pos"
      ],
      "metadata": {
        "id": "FxRKyqkTNjC_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 2 Solution"
      ],
      "metadata": {
        "id": "y8Zn5zbdNwmZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttentionSolution(nn.Module):\n",
        "    num_heads: int\n",
        "    head_dim: int\n",
        "\n",
        "    def setup(self):\n",
        "        self.proj_q = nn.Dense(self.num_heads * self.head_dim)\n",
        "        self.proj_k = nn.Dense(self.num_heads * self.head_dim)\n",
        "        self.proj_v = nn.Dense(self.num_heads * self.head_dim)\n",
        "        self.proj_out = nn.Dense(self.num_heads * self.head_dim)\n",
        "\n",
        "    def __call__(self, x):\n",
        "        B, T, C = x.shape\n",
        "        q = self.proj_q(x).reshape(B, T, self.num_heads, self.head_dim)\n",
        "        k = self.proj_k(x).reshape(B, T, self.num_heads, self.head_dim)\n",
        "        v = self.proj_v(x).reshape(B, T, self.num_heads, self.head_dim)\n",
        "        attn = jnp.einsum(\"bqhd,bkhd->bhqk\", q, k) / jnp.sqrt(self.head_dim)\n",
        "        attn = jax.nn.softmax(attn, axis=-1)\n",
        "        out = jnp.einsum(\"bhqk,bkhd->bqhd\", attn, v).reshape(B, T, -1)\n",
        "        return self.proj_out(out)"
      ],
      "metadata": {
        "id": "K8csGS0tNyFt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 3 Solution"
      ],
      "metadata": {
        "id": "NzXdSOPEOUjk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@jax.jit  # <-- Add this decorator\n",
        "def train_step(state, x, y):\n",
        "    def loss_fn(params):\n",
        "        logits = state.apply_fn(params, x)\n",
        "        loss = optax.softmax_cross_entropy_with_integer_labels(logits, y).mean()\n",
        "        return loss\n",
        "    loss, grads = jax.value_and_grad(loss_fn)(state.params)\n",
        "    return state.apply_gradients(grads=grads), loss"
      ],
      "metadata": {
        "id": "lehI53S3OXGi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tests"
      ],
      "metadata": {
        "id": "Kd9DtLaJOnaU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TestTasks(unittest.TestCase):\n",
        "    def test_positional_encoding(self):\n",
        "        model = Task1Solution(vocab_size=10, hidden_dim=8, block_size=16)\n",
        "        x = jnp.ones((2, 16), dtype=jnp.int32)\n",
        "        params = model.init(jax.random.PRNGKey(0), x)\n",
        "        output = model.apply(params, x)\n",
        "        self.assertEqual(output.shape, (2, 16, 8))\n",
        "\n",
        "# Run tests\n",
        "unittest.main(argv=[''], exit=False)"
      ],
      "metadata": {
        "id": "jmDoeNXgOoxQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}