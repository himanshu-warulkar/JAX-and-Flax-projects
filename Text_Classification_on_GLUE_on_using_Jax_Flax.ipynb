{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/himanshu-warulkar/JAX-and-Flax-projects/blob/main/Text_Classification_on_GLUE_on_using_Jax_Flax.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rEJBSTyZIrIb"
      },
      "source": [
        "# Fine-tuning a Transformers model on TPU with **Flax/JAX**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kTCFado4IrIc"
      },
      "source": [
        "In this notebook, we will see how to fine-tune one of the [Transformers](https://github.com/huggingface/transformers) models on TPU using [**Flax**](https://flax.readthedocs.io/en/latest/index.html). As can be seen on [this benchmark](https://github.com/huggingface/transformers/tree/master/examples/flax/text-classification#runtime-evaluation) using Flax/JAX on GPU/TPU is often much faster and can also be considerably cheaper than using PyTorch on GPU/TPU.\n",
        "\n",
        "[**Flax**](https://flax.readthedocs.io/en/latest/index.html) is a high-performance neural network library designed for flexibility built on top of JAX (see below). It aims to provide users with full control of their training code and is carefully designed to work well with JAX transformations such as `grad` and `pmap` (see the [Flax philosophy](https://flax.readthedocs.io/en/latest/philosophy.html)). For an introduction to Flax see the [Flax Basic Colab](https://flax.readthedocs.io/en/latest/notebooks/flax_basics.html) or the list of curated [Flax examples](https://flax.readthedocs.io/en/latest/examples.html).\n",
        "\n",
        "[**JAX**](https://jax.readthedocs.io/en/latest/index.html) is Autograd and XLA, brought together for high-performance numerical computing and machine learning research. It provides composable transformations of Python+NumPy programs: differentiate, vectorize, parallelize, Just-In-Time compile to GPU/TPU, and more. A great place for getting started with JAX is the [JAX 101 Tutorial](https://jax.readthedocs.io/en/latest/jax-101/index.html)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X4cRE8IbIrIV"
      },
      "source": [
        "If you're opening this Notebook on colab, you will probably need to install  Transformers and  Datasets as well as [Flax](https://github.com/google/flax.git) and [Optax](https://github.com/deepmind/optax). Optax is a gradient processing and optimization library for JAX, and is the optimizer library\n",
        "recommended by Flax."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MOsHUjgdIrIW"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install datasets\n",
        "!pip install git+https://github.com/huggingface/transformers.git\n",
        "!pip install flax\n",
        "!pip install git+https://github.com/deepmind/optax.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EwRqMUxqzOKN"
      },
      "source": [
        "You also will need to set up the TPU for JAX in this notebook. This can be done by executing the following lines."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "drej_4loxdCF"
      },
      "outputs": [],
      "source": [
        "import jax.tools.colab_tpu\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bAIlBLFXXix7"
      },
      "source": [
        "If everything is set up correctly, the following command should return a list of 8 TPU devices."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example: Benchmarking JAX vs PyTorch\n",
        "def benchmark():\n",
        "    # JAX\n",
        "    jax_time = %timeit -o jax_model.apply(params, inputs)\n",
        "    # PyTorch\n",
        "    torch_time = %timeit -o torch_model(inputs)\n",
        "    print(f\"JAX speedup: {torch_time.average / jax_time.average:.2f}x\")"
      ],
      "metadata": {
        "id": "6_T25RAvbqH8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kc0dzzrsXktW"
      },
      "outputs": [],
      "source": [
        "jax.local_devices()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HFASsisvIrIb"
      },
      "source": [
        "If you're opening this notebook locally, make sure your environment has an install from the last version of those libraries.\n",
        "\n",
        "You can find a script version of this notebook to fine-tune your model [here](https://github.com/huggingface/transformers/tree/master/examples/flax/text-classification)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DlpF5MnonmmQ"
      },
      "source": [
        "As an example, we will fine-tune a pretrained [*auto-encoding model*](https://huggingface.co/transformers/model_summary.html#autoencoding-models) on a text classification task of the [GLUE Benchmark](https://gluebenchmark.com/). Note that this notebook does not focus so much on data preprocessing, but rather on how to write a training and evaluation loop in **JAX/Flax**. If you want more detailed explanations regarding the data preprocessing, please check out [this](https://colab.research.google.com/github/huggingface/notebooks/blob/master/examples/text_classification.ipynb#scrollTo=545PP3o8IrJV) notebook.\n",
        "\n",
        "The GLUE Benchmark is a group of nine classification tasks on sentences or pairs of sentences which are:\n",
        "[CoLA](https://nyu-mll.github.io/CoLA/), [MNLI](https://arxiv.org/abs/1704.05426), [MRPC](https://www.microsoft.com/en-us/download/details.aspx?id=52398), [QNLI](https://rajpurkar.github.io/SQuAD-explorer/), [QQP](https://data.quora.com/First-Quora-Dataset-Release-Question-Pairs), [RTE](https://aclweb.org/aclwiki/Recognizing_Textual_Entailment), [SST-2](https://nlp.stanford.edu/sentiment/index.html), [STS-B](http://ixa2.si.ehu.es/stswiki/index.php/STSbenchmark), [WNLI](https://cs.nyu.edu/faculty/davise/papers/WinogradSchemas/WS.html).\n",
        "\n",
        "We will see how to easily load the dataset for each one of those tasks and how to write a training loop in Flax. Each task is named by its acronym, with `mnli-mm` standing for the mismatched version of MNLI (so same training set as `mnli` but different validation and test sets):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YZbiBDuGIrId"
      },
      "outputs": [],
      "source": [
        "GLUE_TASKS = [\"cola\", \"mnli\", \"mnli-mm\", \"mrpc\", \"qnli\", \"qqp\", \"rte\", \"sst2\", \"stsb\", \"wnli\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4RRkXuteIrIh"
      },
      "source": [
        "This notebook is built to run on any of the tasks in the list above, with any Flax/JAX model checkpoint from the [Model Hub](https://huggingface.co/models) as long as that model has a version with a classification head. Depending on the model you are using, you might need to adjust the batch size to avoid out-of-memory errors. Set those three parameters, then the rest of the notebook should run smoothly:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zVvslsfMIrIh"
      },
      "outputs": [],
      "source": [
        "task = \"cola\"\n",
        "model_checkpoint = \"bert-base-cased\"\n",
        "per_device_batch_size = 4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WfCl-6xBYjRu"
      },
      "source": [
        "We also quickly upload some telemetry - this tells us which examples and software versions are getting used so we know where to prioritize our maintenance efforts. We don't collect (or care about) any personally identifiable information, but if you'd prefer not to be counted, feel free to skip this step or delete this cell entirely."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6BfwhiFAYjRu"
      },
      "outputs": [],
      "source": [
        "from transformers.utils import send_example_telemetry\n",
        "\n",
        "send_example_telemetry(\"text_classification_notebook\", framework=\"flax\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "whPRbBNbIrIl"
      },
      "source": [
        "## Loading the dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets --upgrade\n",
        "!pip install matplotlib-venn\n",
        "!apt-get -qq install -y libfluidsynth1\n",
        "!apt-get -qq install -y libarchive-dev && pip install -U libarchive\n",
        "import libarchive\n",
        "!apt-get -qq install -y graphviz && pip install pydot\n",
        "import pydot\n",
        "!apt-get -qq install -y graphviz && pip install pydot\n",
        "import pydot"
      ],
      "metadata": {
        "id": "1vfA3WYVZjEk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W7QYTpxXIrIl"
      },
      "source": [
        "We will use the [Datasets](https://github.com/huggingface/datasets) library to download the data and get the metric we need to use for evaluation (to compare our model to the benchmark). This can be easily done with the functions `load_dataset` and `load_metric`.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IreSlFmlIrIm"
      },
      "outputs": [],
      "source": [
        "!pip install datasets --upgrade\n",
        "from datasets import load_dataset\n",
        "from datasets import load_dataset_builder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CKx2zKs5IrIq"
      },
      "source": [
        "Apart from `mnli-mm` being a special code, we can directly pass our task name to those functions. `load_dataset` will cache the dataset to avoid downloading it again the next time you run this cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s_AY1ATSIrIq"
      },
      "outputs": [],
      "source": [
        "actual_task = \"mnli\" if task == \"mnli-mm\" else task\n",
        "is_regression = task == \"stsb\"\n",
        "\n",
        "raw_dataset = load_dataset(\"glue\", actual_task)\n",
        "metric = load_dataset_builder('glue', actual_task)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n9qywopnIrJH"
      },
      "source": [
        "## Preprocessing the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YVx71GdAIrJH"
      },
      "source": [
        "Before we can feed those texts to our model, we need to preprocess them. This is done by a Transformers `Tokenizer` which will (as the name indicates) tokenize the inputs. This includes converting the tokens to their corresponding IDs in the pretrained vocabulary and putting them in a format the model expects, as well as generate the other inputs that the model requires.\n",
        "\n",
        "To do all of this, we instantiate our tokenizer with the `AutoTokenizer.from_pretrained` method, which will ensure:\n",
        "\n",
        "- we get a tokenizer that corresponds to the model architecture we want to use,\n",
        "- we download the vocabulary used when pretraining this specific checkpoint.\n",
        "\n",
        "That vocabulary will be cached, so it's not downloaded again the next time we run the cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eXNLu_-nIrJI"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qo_0B1M2IrJM"
      },
      "source": [
        "To preprocess our dataset, we will thus need the names of the columns containing the sentence(s). The following dictionary keeps track of the correspondence task to column names:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fyGdtK9oIrJM"
      },
      "outputs": [],
      "source": [
        "task_to_keys = {\n",
        "    \"cola\": (\"sentence\", None),\n",
        "    \"mnli\": (\"premise\", \"hypothesis\"),\n",
        "    \"mnli-mm\": (\"premise\", \"hypothesis\"),\n",
        "    \"mrpc\": (\"sentence1\", \"sentence2\"),\n",
        "    \"qnli\": (\"question\", \"sentence\"),\n",
        "    \"qqp\": (\"question1\", \"question2\"),\n",
        "    \"rte\": (\"sentence1\", \"sentence2\"),\n",
        "    \"sst2\": (\"sentence\", None),\n",
        "    \"stsb\": (\"sentence1\", \"sentence2\"),\n",
        "    \"wnli\": (\"sentence1\", \"sentence2\"),\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2C0hcmp9IrJQ"
      },
      "source": [
        "We can then write the function that will preprocess our samples. We just feed them to the `tokenizer` with the argument `truncation=True`. This will ensure that an input longer than what the model can handle will be truncated to the maximum length accepted by the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FgjEr0fKMRQ_"
      },
      "outputs": [],
      "source": [
        "sentence1_key, sentence2_key = task_to_keys[task]\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    texts = (\n",
        "        (examples[sentence1_key],) if sentence2_key is None else (examples[sentence1_key], examples[sentence2_key])\n",
        "    )\n",
        "    processed = tokenizer(*texts, padding=\"max_length\", max_length=128, truncation=True)\n",
        "\n",
        "    processed[\"labels\"] = examples[\"label\"]\n",
        "    return processed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zS-6iXTkIrJT"
      },
      "source": [
        "To apply this function to all the sentences (or pairs of sentences) in our dataset, we just use the `map` method of our dataset object we created earlier. This will apply the function on all the elements of all the splits in the dataset, so our training, validation, and testing data will be preprocessed in one single command.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DDtsaJeVIrJT"
      },
      "outputs": [],
      "source": [
        "tokenized_dataset = raw_dataset.map(preprocess_function, batched=True, remove_columns=raw_dataset[\"train\"].column_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "voWiw8C7IrJV"
      },
      "source": [
        "As a final step, we split the dataset into the *train* and *validation* dataset and give each set more explicit names."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K03vmCmPqjIP"
      },
      "outputs": [],
      "source": [
        "train_dataset = tokenized_dataset[\"train\"]\n",
        "eval_dataset = tokenized_dataset[\"validation\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "545PP3o8IrJV"
      },
      "source": [
        "## Fine-tuning the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FBiW8UpKIrJW"
      },
      "source": [
        "Now that our data is ready, we can download the pretrained model and fine-tune it. Since all our tasks are about sentence classification, we use the `FlaxAutoModelForSequenceClassification` class. Like with the tokenizer, the `from_pretrained` method will download and cache the model for us.\n",
        "\n",
        "All weight parameters that are not found in the pretrained model weights will be randomly initialized upon instantiating the model class. Because the GLUE task contains relatively small training datasets, a different seed for weight initialization might very well lead to significantly different results. For\n",
        "reproducibility, we set the random seed to *0* in this notebook.\n",
        "\n",
        "The only thing we have to specify in the config is the number of labels for our problem (which is always 2, except for STS-B which is a regression problem, and MNLI where we have 3 labels):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TlqNaB8jIrJW"
      },
      "outputs": [],
      "source": [
        "from transformers import FlaxAutoModelForSequenceClassification, AutoConfig\n",
        "\n",
        "num_labels = 3 if task.startswith(\"mnli\") else 1 if task==\"stsb\" else 2\n",
        "seed = 0\n",
        "\n",
        "config = AutoConfig.from_pretrained(model_checkpoint, num_labels=num_labels)\n",
        "model = FlaxAutoModelForSequenceClassification.from_pretrained(model_checkpoint, config=config, seed=seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CczA5lJlIrJX"
      },
      "source": [
        "The warning is telling us we are throwing away some weights (the `vocab_transform` and `vocab_layer_norm` layers) and randomly initializing some others (the `pre_classifier` and `classifier` layers). This is normal in this case because we are removing the head used to pretrain the model on a masked language modeling objective and replacing it with a new head for which we don't have pretrained weights, so the library warns us we should fine-tune this model before using it for inference, which is exactly what we are going to do."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dnQ43MgRqV-K"
      },
      "source": [
        "To write a full training and evaluation loop in Flax, we will need to import a couple of packages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mn1GdGpipfWK"
      },
      "outputs": [],
      "source": [
        "import flax\n",
        "import jax\n",
        "import optax\n",
        "\n",
        "from itertools import chain\n",
        "from tqdm.notebook import tqdm\n",
        "from typing import Callable\n",
        "\n",
        "import jax.numpy as jnp\n",
        "\n",
        "from flax.training.common_utils import get_metrics, onehot, shard, shard_prng_key\n",
        "from flax.training import train_state"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I3GOlfPQrRdY"
      },
      "source": [
        "For all GLUE tasks except `\"wnli\"` and `\"mrpc\"`, it is usually sufficient to train for just 3 epochs. `\"wnli\"` and `\"mrpc\"` are so small that we recommend training on 5 epochs. We use a learning rate of *0.00002*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8zpJOCJhmSno"
      },
      "outputs": [],
      "source": [
        "num_train_epochs = 3 if task not in [\"mrpc\", \"wnli\"] else 5\n",
        "learning_rate = 2e-5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K7mQfa25tAFb"
      },
      "source": [
        "We've already set the batch size per device, but are now interested in the effective total batch_size:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QfP-yW52s-lu"
      },
      "outputs": [],
      "source": [
        "total_batch_size = per_device_batch_size * jax.local_device_count()\n",
        "print(\"The overall batch size (both for training and eval) is\", total_batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CDQbpTFvtpyN"
      },
      "source": [
        "Next, we define the learning rate schedule. A simple and effective learning rate schedule is the linear decay with warmup (click [here](https://huggingface.co/transformers/main_classes/optimizer_schedules.html#transformers.get_linear_schedule_with_warmup) for more information). Since GLUE datasets are rather small and therefore have few training steps, we set the number of warmup steps simply to 0. The schedule is then fully defined by the number of training steps and the learning rate.\n",
        "\n",
        "It is recommended to use the [**optax**](https://github.com/deepmind/optax) library for training utilities, *e.g.* learning rate schedules and optimizers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0vnrOjubxkFw"
      },
      "outputs": [],
      "source": [
        "num_train_steps = len(train_dataset) // total_batch_size * num_train_epochs\n",
        "\n",
        "learning_rate_function = optax.linear_schedule(init_value=learning_rate, end_value=0, transition_steps=num_train_steps)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xorWAbYWMfEm"
      },
      "source": [
        "### Defining the training state"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ktzA1CJAyrZB"
      },
      "source": [
        "Next, we will create the *training state* that includes the optimizer, the loss function, and is responsible for updating the model's parameters during training.\n",
        "\n",
        "Most JAX transformations (notably [jax.jit](https://jax.readthedocs.io/en/latest/jax-101/02-jitting.html)) require functions that are transformed to have no side-effects. This is because any such side-effects will only be executed once, when the Python version of the function is run during compilation (see [Stateful Computations in JAX](https://jax.readthedocs.io/en/latest/jax-101/07-state.html)). As a consequence, Flax models (which can be transformed by JAX transformations) are **immutable**, and the state of the model (i.e., its weight parameters) are stored *outside* of the model instance.\n",
        "\n",
        "Models are initialized and updated in a purely functional way: you pass the state to the model when calling it, and the model returns the new (possibly modified) state, leaving the model instance itself unchanged.\n",
        "\n",
        "Flax provides a convenience class [`flax.training.train_state.TrainState`](https://github.com/google/flax/blob/9da95cdd12591f42d2cd4c17089861bff7e43cc5/flax/training/train_state.py#L22), which stores things such as the model parameters, the loss function, the optimizer, and exposes an `apply_gradients` function to update the model's weight parameters.\n",
        "\n",
        "Alright, let's begin by defining our *training state* class. We create a derived `TrainState` class that additionally stores the model's forward pass as `eval_function` as well as a `loss_function`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7F81TyR3nJrM"
      },
      "outputs": [],
      "source": [
        "class TrainState(train_state.TrainState):\n",
        "    logits_function: Callable = flax.struct.field(pytree_node=False)\n",
        "    loss_function: Callable = flax.struct.field(pytree_node=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DRF_WotzGOpR"
      },
      "source": [
        "We will be using the standard Adam optimizer with weight decay. For more information on AdamW (Adam + weight decay), one can take a look at [this](https://www.fast.ai/2018/07/02/adam-weight-decay/) blog post.\n",
        "\n",
        "AdamW can easily be imported from [optax](https://github.com/deepmind/optax):"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FlYAEe58JPdy"
      },
      "source": [
        "Regularizing the *bias* and/or *LayerNorm* has not shown to improve performance and can even be disadvantageous, which is why we disable it here. For more information on this, please check out the following [blog post](https://medium.com/@shrutijadon10104776/why-we-dont-use-bias-in-regularization-5a86905dfcd6) or [paper](https://arxiv.org/abs/1711.05101).\n",
        "\n",
        "Hence we create a `decay_mask_fn` which makes sure that weight decay is not applied to any *bias* or *LayerNorm* weights. This can easily be done by passing a `mask_fn` to `optax.adamw`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ap-zaOyKJDXM"
      },
      "outputs": [],
      "source": [
        "from flax import traverse_util\n",
        "\n",
        "def decay_mask_fn(params):\n",
        "    flat_params = traverse_util.flatten_dict(params)\n",
        "    flat_mask = {path: (path[-1] != \"bias\" and path[-2:] != (\"LayerNorm\", \"scale\")) for path in flat_params}\n",
        "    return traverse_util.unflatten_dict(flat_mask)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lku54GQWq_6v"
      },
      "outputs": [],
      "source": [
        "import optax\n",
        "\n",
        "def adamw(weight_decay):\n",
        "    return optax.adamw(learning_rate=learning_rate_function, b1=0.9, b2=0.999, eps=1e-6, weight_decay=weight_decay, mask=decay_mask_fn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0vbf4KyrKnDl"
      },
      "source": [
        "Now we also need to define the evaluation and loss function given the model's output logits.\n",
        "For regression tasks, the evaluation function simply takes the first logit element and the mean-square error (mse) loss is used. For classification tasks, the evaluation function uses the `argmax` of the logits, and the cross-entropy loss with `num_labels` is used."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Rj31RyjJ_9T"
      },
      "outputs": [],
      "source": [
        "def loss_function(logits, labels):\n",
        "  if is_regression:\n",
        "    return jnp.mean((logits[..., 0] - labels) ** 2)\n",
        "\n",
        "  xentropy = optax.softmax_cross_entropy(logits, onehot(labels, num_classes=num_labels))\n",
        "  return jnp.mean(xentropy)\n",
        "\n",
        "def eval_function(logits):\n",
        "    return logits[..., 0] if is_regression else logits.argmax(-1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XM7LmtIRHWFl"
      },
      "source": [
        "Finally, we put the pieces together to instantiate a `TrainState`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pqK0fDCvmxao"
      },
      "outputs": [],
      "source": [
        "state = TrainState.create(\n",
        "    apply_fn=model.__call__,\n",
        "    params=model.params,\n",
        "    tx=adamw(weight_decay=0.01),\n",
        "    logits_function=eval_function,\n",
        "    loss_function=loss_function,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HQHbpxNjMpXb"
      },
      "source": [
        "### Defining the training and evaluation step\n",
        "\n",
        "During fine-tuning, we want to update the model parameters and evaluate the performance after each epoch.\n",
        "\n",
        "Let's write the functions `train_step` and `eval_step` accordingly. During training the weight parameters should be updated as follows:\n",
        "\n",
        "1. Define a loss function `loss_function` that first runs a forward pass of the model given data input. Remember that Flax models are immutable, and we explicitly pass it the state (in this case the model parameters and the RNG). `loss_function` returns a scalar loss (using the previously defined `state.loss_function`) between the model output and input targets.\n",
        "2. Differentiate this loss function using [`jax.value_and_grad`](https://jax.readthedocs.io/en/latest/notebooks/autodiff_cookbook.html#evaluate-a-function-and-its-gradient-using-value-and-grad). This is a JAX transformation called [automatic differentiation](https://en.wikipedia.org/wiki/Automatic_differentiation), which computes the gradient of `loss_function` given the input to the function (i.e., the parameters of the model), and returns the value and the gradient in a pair `(loss, gradients)`.\n",
        "3. Compute the mean gradient over all devices using the collective operation [lax.pmean](https://jax.readthedocs.io/en/latest/_autosummary/jax.lax.pmean.html). As we will see below, each device runs `train_step` on a different batch of data, but by taking the mean here we ensure the model parameters are the same on all devices.\n",
        "4. Use `state.apply_gradients`, which applies the gradients to the weights.\n",
        "\n",
        "Below, you can see how each of the described steps above is put into practice."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QXDgjdbzn3QA"
      },
      "outputs": [],
      "source": [
        "def train_step(state, batch, dropout_rng):\n",
        "    targets = batch.pop(\"labels\")\n",
        "    dropout_rng, new_dropout_rng = jax.random.split(dropout_rng)\n",
        "\n",
        "    def loss_function(params):\n",
        "        logits = state.apply_fn(**batch, params=params, dropout_rng=dropout_rng, train=True)[0]\n",
        "        loss = state.loss_function(logits, targets)\n",
        "        return loss\n",
        "\n",
        "    grad_function = jax.value_and_grad(loss_function)\n",
        "    loss, grad = grad_function(state.params)\n",
        "    grad = jax.lax.pmean(grad, \"batch\")\n",
        "    new_state = state.apply_gradients(grads=grad)\n",
        "    metrics = jax.lax.pmean({\"loss\": loss, \"learning_rate\": learning_rate_function(state.step)}, axis_name=\"batch\")\n",
        "    return new_state, metrics, new_dropout_rng"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lfiPHXYiSvSk"
      },
      "source": [
        "Now, we want to do parallelized training over all TPU devices. To do so, we use [`jax.pmap`](https://jax.readthedocs.io/en/latest/jax.html?highlight=pmap#parallelization-pmap). This will compile the function once and run the same program on each device (it is an [SPMD program](https://en.wikipedia.org/wiki/SPMD)). When calling this pmapped function, all inputs (`\"state\"`, `\"batch\"`, `\"dropout_rng\"`) should be replicated for all devices, which means that the first axis of each argument is used to map over all TPU devices.\n",
        "\n",
        "The argument `donate_argnums` is used to tell JAX that the first argument `\"state\"` is \"donated\" to the computation, because it is not needed anymore afterwards. XLA can make use of donated buffers to reduce the memory needed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t6eLQN4Jn6jS"
      },
      "outputs": [],
      "source": [
        "parallel_train_step = jax.pmap(train_step, axis_name=\"batch\", donate_argnums=(0,))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pu5G-8lfVNzt"
      },
      "source": [
        "Similarly, we can now define the evaluation step. Here, the function is much easier as it simply needs to stack the model's forward pass with the previously defined `eval_function` (or `logits_function`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "chY-zaUln94G"
      },
      "outputs": [],
      "source": [
        "def eval_step(state, batch):\n",
        "    logits = state.apply_fn(**batch, params=state.params, train=False)[0]\n",
        "    return state.logits_function(logits)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nHdL0Op8Vrdu"
      },
      "source": [
        "We then also apply `jax.pmap` to the evaluation step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9P8DBfk2oA_e"
      },
      "outputs": [],
      "source": [
        "parallel_eval_step = jax.pmap(eval_step, axis_name=\"batch\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b99v2GVjWCRr"
      },
      "source": [
        "### Defining the data collators\n",
        "\n",
        "In a final step before we can start training, we need to define the data collators. The data collator is important to shuffle the training data before each epoch and to prepare the batch for each training and evaluation step.\n",
        "\n",
        "Let's start with the training collator."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J7jyyxibXGks"
      },
      "source": [
        "The training collator can be defined as a [Python generator](https://wiki.python.org/moin/Generators) that returns a batch model input every time it is called.\n",
        "\n",
        "First, a random permutation of the whole dataset is defined.\n",
        "Then, every time the training data collator is called the next batch of the randomized dataset is extracted, converted to a JAX array and sharded over all local TPU devices."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YZ6iVzivl88S"
      },
      "outputs": [],
      "source": [
        "def glue_train_data_loader(rng, dataset, batch_size):\n",
        "    steps_per_epoch = len(dataset) // batch_size\n",
        "    perms = jax.random.permutation(rng, len(dataset))\n",
        "    perms = perms[: steps_per_epoch * batch_size]  # Skip incomplete batch.\n",
        "    perms = perms.reshape((steps_per_epoch, batch_size))\n",
        "\n",
        "    for perm in perms:\n",
        "        batch = dataset[perm]\n",
        "        batch = {k: jnp.array(v) for k, v in batch.items()}\n",
        "        batch = shard(batch)\n",
        "\n",
        "        yield batch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bOSPOyMYZt29"
      },
      "source": [
        "We define the eval data collator in a similar fashion.\n",
        "\n",
        "**Note**:\n",
        "For simplicity, we throw away the last incomplete batch since it can't be easily sharded over all devices. This means that the evaluation results might be slightly incorrect. It can be easily fixed by including [this](https://github.com/huggingface/transformers/blob/f063c56d942737d2c7aac93895cd8310afd9c7a4/examples/flax/text-classification/run_flax_glue.py#L491) part in the training loop after evaluation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cZpbf9pamCEW"
      },
      "outputs": [],
      "source": [
        "def glue_eval_data_loader(dataset, batch_size):\n",
        "    for i in range(len(dataset) // batch_size):\n",
        "        batch = dataset[i * batch_size : (i + 1) * batch_size]\n",
        "        batch = {k: jnp.array(v) for k, v in batch.items()}\n",
        "        batch = shard(batch)\n",
        "\n",
        "        yield batch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vf_eG7OBbGAJ"
      },
      "source": [
        "Next, we replicate/copy the weight parameters on each device, so that we can pass them to our pmapped functions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r1sA7cjySB2T"
      },
      "outputs": [],
      "source": [
        "state = flax.jax_utils.replicate(state)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I42ZJzpgbo-s"
      },
      "source": [
        "### Training\n",
        "\n",
        "Finally, we can write down the full training loop."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bq40T7_Wbw2p"
      },
      "source": [
        "Let's start by generating a seeded `PRNGKey` for the dropout layers and dataset shuffling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Wyaedrnty4-"
      },
      "outputs": [],
      "source": [
        "rng = jax.random.PRNGKey(seed)\n",
        "dropout_rngs = jax.random.split(rng, jax.local_device_count())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xByhGKVUcquw"
      },
      "source": [
        "Now we define the full training loop. For each batch in each epoch, we run a training step. Here, we also need to make sure that the PRNGKey is sharded/split over each device. Having completed an epoch, we report the training metrics and can run the evaluation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FBARxJ8soOrs"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset, load_dataset_builder\n",
        "from transformers import AutoTokenizer, FlaxAutoModelForSequenceClassification, AutoConfig\n",
        "import flax\n",
        "import jax\n",
        "import optax\n",
        "import jax.numpy as jnp\n",
        "from flax.training.common_utils import get_metrics, onehot, shard, shard_prng_key\n",
        "from flax.training import train_state\n",
        "from itertools import chain\n",
        "from tqdm.notebook import tqdm\n",
        "from typing import Callable\n",
        "\n",
        "# ... (rest of your existing code) ...\n",
        "\n",
        "# Load the dataset and metric information\n",
        "actual_task = \"mnli\" if task == \"mnli-mm\" else task\n",
        "is_regression = task == \"stsb\"\n",
        "raw_dataset = load_dataset(\"glue\", actual_task)\n",
        "\n",
        "# Use load_dataset_builder to get the dataset info\n",
        "dataset_builder = load_dataset_builder('glue', actual_task)\n",
        "metric_info = dataset_builder.info.features['label']._type  # Access metric details\n",
        "\n",
        "# Get the metric name and function\n",
        "if metric_info == 'ClassLabel':\n",
        "    metric_name = dataset_builder.info.features['label'].names[0]  # Assuming single label\n",
        "    # Implement custom metric calculation using metric_name or similar approach\n",
        "    # (See example below)\n",
        "elif metric_info == 'Value':\n",
        "    # Handle regression metrics if needed\n",
        "    pass\n",
        "else:\n",
        "    raise ValueError(f\"Unsupported metric type: {metric_info}\")\n",
        "\n",
        "# Example for custom metric calculation (assuming accuracy):\n",
        "def compute_accuracy(predictions, references):\n",
        "    correct_predictions = jnp.sum(jnp.equal(predictions, references))\n",
        "    total_predictions = len(predictions)\n",
        "    accuracy = correct_predictions / total_predictions\n",
        "    return accuracy\n",
        "\n",
        "# Update the evaluation loop:\n",
        "for i, epoch in enumerate(tqdm(range(1, num_train_epochs + 1), desc=f\"Epoch ...\", position=0, leave=True)):\n",
        "    # ... (training loop remains the same) ...\n",
        "\n",
        "    # evaluate\n",
        "    all_predictions = []\n",
        "    all_labels = []\n",
        "    with tqdm(total=len(eval_dataset) // total_batch_size, desc=\"Evaluating...\", leave=False) as progress_bar_eval:\n",
        "        for batch in glue_eval_data_loader(eval_dataset, total_batch_size):\n",
        "            labels = batch.pop(\"labels\")\n",
        "            predictions = parallel_eval_step(state, batch)\n",
        "            all_predictions.extend(chain(*predictions))\n",
        "            all_labels.extend(chain(*labels))\n",
        "            progress_bar_eval.update(1)\n",
        "\n",
        "    # Compute the metric using the custom function\n",
        "    eval_score = compute_accuracy(jnp.array(all_predictions), jnp.array(all_labels))\n",
        "\n",
        "    loss = round(flax.jax_utils.unreplicate(train_metrics)['loss'].item(), 3)\n",
        "    eval_score = round(eval_score.item(), 3)  # Convert to Python scalar\n",
        "\n",
        "    print(f\"{i+1}/{num_train_epochs} | Train loss: {loss} | Eval {metric_name}: {eval_score}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ffP-VQOyIrJk"
      },
      "source": [
        "To see how your model fared you can compare it to the [GLUE Benchmark leaderboard](https://gluebenchmark.com/leaderboard)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2UB8pjv8vxYa"
      },
      "source": [
        "### Sharing fine-tuned model\n",
        "\n",
        "Now that you've succesfully trained a model, you can share it with the community by uploading the fine-tuned model checkpoint and tokenizer to your account on the [hub](https://huggingface.co/models).\n",
        "\n",
        "\n",
        "If you don't have an account yet, you can click [here](https://huggingface.co/join) join the community 🤗."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8vPLVtkS-JHi"
      },
      "source": [
        "In a first step, we install [git-lfs](https://git-lfs.github.com/) to easily upload the model weights."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QOHBpZulqSLT"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!sudo apt-get install software-properties-common\n",
        "!sudo curl -s https://packagecloud.io/install/repositories/github/git-lfs/script.deb.sh | sudo bash\n",
        "!sudo apt-get install git-lfs\n",
        "!git lfs install"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XJduNRXk_V2c"
      },
      "source": [
        "Next, we you will need to store your git credentials so that `git` knows who is uploading the files. You should replace the fields `<your-email-address>` and `<your-name>` with your credentials accordingly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D5ILJ1B-up41"
      },
      "outputs": [],
      "source": [
        "!git config --global user.email \"w.himanshuoffice365@gmail.com\"  # e.g. \"patrick.v.platen@gmail.com\"\n",
        "!git config --global user.name \"himanshu-warulkar\"  # e.g. \"Patrick von Platen\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QwPQyaTOAM5H"
      },
      "source": [
        "You will need to pass your user authentification token `hf_auth_token` to allow the 🤗 hub to upload model weights under your username. To find your authentification token, you need to log in [here](https://huggingface.co/login), click on your icon (top right), go to *Settings*, then *API Tokens*. You can copy the *User API token* and replace it with the field `<your-auth-token>` below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OgET7o-Jv2OR"
      },
      "outputs": [],
      "source": [
        "hf_auth_token = \"\"  # e.g. api_DaYgaaVnGdRtznIgiNfotCHFUqmOdARmPx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FRM1k_8FCMiP"
      },
      "source": [
        "Finally, you can give your fine-tuned model a nice `model_id` or leave the default one as noted below. The model will be uploaded under `https://huggingface.co/<your-username>/<your-model-id>`, *e.g.*\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-XGT5eAzBoUK"
      },
      "outputs": [],
      "source": [
        "model_id = f\"{model_checkpoint}_fine_tuned_glue_{task}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FJHgcqvkBsIc"
      },
      "source": [
        "Great! Now all that is left to do is to upload your model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cIT_aQ-OoxvG"
      },
      "outputs": [],
      "source": [
        "model.push_to_hub(model_id, use_auth_token=hf_auth_token)\n",
        "tokenizer.push_to_hub(model_id, use_auth_token=hf_auth_token)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JlviIJzcC_WD"
      },
      "source": [
        "You can now go to your model page to check it out.\n",
        "\n",
        "We strongly recommend to add a model card so that the community can actually make use of your fine-tuned model. You can do so by clicking on *Create Model Card* and adding a descriptive text in markdown format.\n",
        "\n",
        "A simple description for your fine-tuned model is given by running the following cell. You can simply copy-paste the output to be used as your model card `README.md`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QSKl6rX7D_-N"
      },
      "outputs": [],
      "source": [
        "print(f\"\"\"---\n",
        "language: en\n",
        "license: apache-2.0\n",
        "datasets:\n",
        "- glue\n",
        "---\n",
        "# {\" \".join([x.capitalize() for x in model_id.split(\"_\")])}\n",
        "\n",
        "This checkpoint was initialized from the pre-trained checkpoint {model_checkpoint} and subsequently fine-tuned on GLUE task: {task} using [this](https://colab.research.google.com/drive/162pW3wonGcMMrGxmA-jdxwy1rhqXd90x?usp=sharing) notebook.\n",
        "Training was conducted for {num_train_epochs} epochs, using a linear decaying learning rate of {learning_rate}, and a total batch size of {total_batch_size}.\n",
        "\n",
        "The model has a final training loss of {loss} and a {metric_name} of {eval_score}.\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OpFZeamCI-FJ"
      },
      "source": [
        "An uploaded model would, *e.g.*, look as follows: [patrickvonplaten/bert-base-cased_fine_tuned_glue_mrpc_demo](https://huggingface.co/patrickvonplaten/bert-base-cased_fine_tuned_glue_mrpc_demo)."
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "provenance": [],
      "gpuType": "V28",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}